{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"rotalabs-accel","text":"<p>High-performance inference acceleration for modern LLMs with Triton kernels, INT8 quantization, and automatic CPU/GPU fallback.</p> <ul> <li> <p>:zap: Triton-Optimized Kernels</p> <p>RMSNorm, SwiGLU, RoPE, and INT8 GEMM kernels written in Triton with 3-4x speedups over PyTorch.</p> </li> <li> <p>:package: Drop-in Replacements</p> <p><code>nn.Module</code> classes that match PyTorch's API exactly. Swap one line, get instant speedups.</p> </li> <li> <p>:gear: Automatic Fallback</p> <p>Works everywhere. Uses Triton on CUDA, pure PyTorch on CPU. No code changes needed.</p> </li> <li> <p>:chart_with_upwards_trend: INT8 Quantization</p> <p>Cut memory usage in half with symmetric INT8 weight quantization (W8A16).</p> </li> </ul>"},{"location":"#why-rotalabs-accel","title":"Why rotalabs-accel?","text":"<p>Modern LLMs like LLaMA, Mistral, and Qwen use the same core operations: RMSNorm, SwiGLU, and RoPE. These operations are memory-bound\u2014the GPU spends most of its time moving data, not computing.</p> <p>By fusing operations and writing custom Triton kernels, we eliminate redundant memory traffic and achieve significant speedups:</p> Kernel PyTorch Baseline Triton Kernel Speedup RMSNorm 45 \u03bcs 12 \u03bcs 3.8x SwiGLU 89 \u03bcs 31 \u03bcs 2.9x RoPE 67 \u03bcs 23 \u03bcs 2.9x INT8 GEMM 156 \u03bcs 48 \u03bcs 3.3x <p>Benchmarks on A100-80GB, batch_size=1, seq_len=2048, hidden_size=4096</p>"},{"location":"#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                      rotalabs-accel                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  High-Level API (nn.Module)                                 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502TritonRMSNorm \u2502   SwiGLU     \u2502RotaryEmbedding\u2502Int8Linear\u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Functional API                                             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502   rmsnorm    \u2502 swiglu_fused \u2502  apply_rope  \u2502 int8_gemm \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Dispatch Layer (automatic Triton/PyTorch selection)        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502  Triton Kernels (CUDA)  \u2502 \u2502 PyTorch Reference (CPU/GPU)\u2502 \u2502\n\u2502  \u2502  - _rmsnorm_kernel      \u2502 \u2502  - rmsnorm_torch           \u2502 \u2502\n\u2502  \u2502  - _swiglu_kernel       \u2502 \u2502  - swiglu_torch            \u2502 \u2502\n\u2502  \u2502  - _rope_kernel         \u2502 \u2502  - rope_torch              \u2502 \u2502\n\u2502  \u2502  - _int8_gemm_kernel    \u2502 \u2502  - int8_gemm_torch         \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"#design-principles","title":"Design Principles","text":"<ol> <li> <p>Zero-friction adoption: Every module is a drop-in replacement. <code>TritonRMSNorm</code> has the exact same interface as <code>torch.nn.RMSNorm</code>.</p> </li> <li> <p>Graceful degradation: If Triton isn't available or inputs are on CPU, kernels automatically fall back to pure PyTorch. Your code works everywhere.</p> </li> <li> <p>Numerical stability: All kernels use FP32 accumulation internally, matching PyTorch's numerical behavior.</p> </li> <li> <p>Minimal dependencies: Core package only requires PyTorch. Triton is optional.</p> </li> </ol>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<pre><code># Basic installation (CPU support only)\npip install rotalabs-accel\n\n# With Triton support (recommended for GPU)\npip install rotalabs-accel[triton]\n\n# Full installation with benchmarking tools\npip install rotalabs-accel[triton,benchmark]\n</code></pre>"},{"location":"#basic-usage","title":"Basic Usage","text":"<pre><code>import torch\nfrom rotalabs_accel import TritonRMSNorm, SwiGLU, RotaryEmbedding, get_device\n\n# Auto-detect best device\ndevice = get_device()  # Returns 'cuda' if available, else 'cpu'\n\n# Create optimized layers (same API as PyTorch)\nnorm = TritonRMSNorm(hidden_size=4096, eps=1e-6).to(device)\nffn = SwiGLU(hidden_size=4096, intermediate_size=11008).to(device)\nrope = RotaryEmbedding(dim=128, max_seq_len=8192)\n\n# Forward pass - automatically uses Triton on CUDA\nx = torch.randn(1, 2048, 4096, device=device, dtype=torch.float16)\nx = norm(x)\nx = ffn(x)\n</code></pre>"},{"location":"#int8-quantization","title":"INT8 Quantization","text":"<p>Reduce memory usage by 50% with W8A16 quantization:</p> <pre><code>from rotalabs_accel import QuantizedLinear\n\n# Convert existing linear layer\nlinear = torch.nn.Linear(4096, 4096)\nquantized = QuantizedLinear.from_linear(linear)\n\n# Memory usage: 4096 * 4096 * 2 bytes (FP16) \u2192 4096 * 4096 * 1 byte (INT8)\n# 32 MB \u2192 16 MB per layer\n</code></pre>"},{"location":"#supported-operations","title":"Supported Operations","text":""},{"location":"#normalization","title":"Normalization","text":"Operation Description Use Case <code>rmsnorm</code> Root Mean Square Normalization LLaMA, Mistral, Qwen <code>rmsnorm_residual_fused</code> RMSNorm + residual addition Transformer blocks <code>TritonRMSNorm</code> Drop-in <code>nn.Module</code> replacement Any model using RMSNorm"},{"location":"#activations","title":"Activations","text":"Operation Description Use Case <code>swiglu_fused</code> SiLU(gate) \u00d7 up LLaMA, PaLM, Mistral FFN <code>SwiGLU</code> Full FFN module with projections Transformer FFN replacement"},{"location":"#position-embeddings","title":"Position Embeddings","text":"Operation Description Use Case <code>build_rope_cache</code> Precompute cos/sin tables Initialization <code>apply_rope</code> Apply rotary embeddings to Q/K Attention layers <code>RotaryEmbedding</code> Self-contained <code>nn.Module</code> Attention replacement"},{"location":"#quantization","title":"Quantization","text":"Operation Description Use Case <code>quantize_symmetric</code> Symmetric INT8 quantization Weight compression <code>quantize_weight_per_channel</code> Per-output-channel scales Better accuracy <code>Int8Linear</code> INT8 linear layer Memory-efficient inference <code>QuantizedLinear</code> Higher-level quantized linear Easy model conversion"},{"location":"#integration-examples","title":"Integration Examples","text":""},{"location":"#with-hugging-face-transformers","title":"With Hugging Face Transformers","text":"<pre><code>from transformers import AutoModelForCausalLM\nfrom rotalabs_accel import TritonRMSNorm, SwiGLU\n\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n\n# Replace RMSNorm layers\nfor name, module in model.named_modules():\n    if \"layernorm\" in name.lower() or \"rmsnorm\" in name.lower():\n        parent = model.get_submodule(\".\".join(name.split(\".\")[:-1]))\n        attr = name.split(\".\")[-1]\n        setattr(parent, attr, TritonRMSNorm(module.weight.shape[0]))\n</code></pre>"},{"location":"#with-vllm-custom-kernels","title":"With vLLM Custom Kernels","text":"<pre><code># rotalabs-accel kernels can be used as building blocks for custom models\nfrom rotalabs_accel import rmsnorm, swiglu_fused, apply_rope\n\nclass OptimizedLlamaBlock(torch.nn.Module):\n    def forward(self, x, residual, cos, sin):\n        # Fused residual + RMSNorm\n        x = rmsnorm_residual_fused(x, residual, self.ln_weight)\n\n        # Attention with RoPE\n        q, k, v = self.qkv_proj(x).chunk(3, dim=-1)\n        q, k = apply_rope(q, k, cos, sin)\n        ...\n</code></pre>"},{"location":"#device-capabilities","title":"Device Capabilities","text":"<p>Check what's available on your system:</p> <pre><code>from rotalabs_accel import (\n    get_device,\n    is_cuda_available,\n    is_triton_available,\n    get_device_properties,\n)\n\nprint(f\"CUDA available: {is_cuda_available()}\")\nprint(f\"Triton available: {is_triton_available()}\")\n\nif is_cuda_available():\n    props = get_device_properties()\n    print(f\"GPU: {props['name']}\")\n    print(f\"Compute capability: {props['compute_capability']}\")\n    print(f\"Supports BF16: {props['supports_bf16']}\")\n    print(f\"Supports FP8: {props['supports_fp8']}\")\n</code></pre>"},{"location":"#roadmap","title":"Roadmap","text":"<ul> <li>[ ] FP8 quantization for Hopper/Blackwell GPUs</li> <li>[ ] Asymmetric INT4 quantization (GPTQ-style)</li> <li>[ ] EAGLE-style speculative decoding</li> <li>[ ] Flash Attention integration</li> <li>[ ] KV cache compression</li> <li>[ ] CUDA graphs for static workloads</li> </ul>"},{"location":"#links","title":"Links","text":"<ul> <li>GitHub Repository</li> <li>PyPI Package</li> <li>Rotalabs Website</li> </ul>"},{"location":"#license","title":"License","text":"<p>MIT License - see LICENSE for details.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This guide covers installation, basic usage, and best practices for rotalabs-accel.</p>"},{"location":"getting-started/#installation","title":"Installation","text":""},{"location":"getting-started/#requirements","title":"Requirements","text":"<ul> <li>Python 3.9+</li> <li>PyTorch 2.0+</li> <li>(Optional) CUDA 11.8+ for GPU acceleration</li> <li>(Optional) Triton 2.1+ for optimized kernels</li> </ul>"},{"location":"getting-started/#installation-options","title":"Installation Options","text":"Basic (CPU only)With Triton (Recommended)Full InstallationFrom Source <pre><code>pip install rotalabs-accel\n</code></pre> <p>Core functionality with PyTorch-only implementations. Works on any platform.</p> <pre><code>pip install rotalabs-accel[triton]\n</code></pre> <p>Enables Triton-optimized kernels for 3-4x speedups on NVIDIA GPUs.</p> <pre><code>pip install rotalabs-accel[triton,benchmark,dev]\n</code></pre> <p>Includes benchmarking tools and development dependencies.</p> <pre><code>git clone https://github.com/rotalabs/rotalabs-accel.git\ncd rotalabs-accel\npip install -e \".[triton,dev]\"\n</code></pre>"},{"location":"getting-started/#verify-installation","title":"Verify Installation","text":"<pre><code>import rotalabs_accel\nprint(f\"Version: {rotalabs_accel.__version__}\")\n\nfrom rotalabs_accel import is_cuda_available, is_triton_available\nprint(f\"CUDA available: {is_cuda_available()}\")\nprint(f\"Triton available: {is_triton_available()}\")\n</code></pre>"},{"location":"getting-started/#core-concepts","title":"Core Concepts","text":""},{"location":"getting-started/#automatic-dispatch","title":"Automatic Dispatch","text":"<p>Every function in rotalabs-accel automatically selects the best implementation:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    rmsnorm(x, weight)                        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Is x on CUDA?                                               \u2502\n\u2502  \u251c\u2500\u2500 YES: Is Triton installed?                              \u2502\n\u2502  \u2502   \u251c\u2500\u2500 YES: Use Triton kernel      \u2190 Fastest              \u2502\n\u2502  \u2502   \u2514\u2500\u2500 NO:  Use PyTorch on GPU                            \u2502\n\u2502  \u2514\u2500\u2500 NO:  Use PyTorch on CPU         \u2190 Universal fallback   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>You don't need to check conditions or write platform-specific code\u2014just call the function.</p>"},{"location":"getting-started/#module-vs-functional-api","title":"Module vs Functional API","text":"<p>rotalabs-accel provides two ways to use each operation:</p> <p>Module API - For use in <code>nn.Module</code> classes:</p> <pre><code>from rotalabs_accel import TritonRMSNorm, SwiGLU\n\nclass MyModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.norm = TritonRMSNorm(4096)\n        self.ffn = SwiGLU(4096, 11008)\n\n    def forward(self, x):\n        return self.ffn(self.norm(x))\n</code></pre> <p>Functional API - For direct tensor operations:</p> <pre><code>from rotalabs_accel import rmsnorm, swiglu_fused\n\ndef forward(x, weight, gate_weight, up_weight, down_weight):\n    x = rmsnorm(x, weight)\n    gate = x @ gate_weight.T\n    up = x @ up_weight.T\n    return swiglu_fused(gate, up) @ down_weight.T\n</code></pre>"},{"location":"getting-started/#using-the-kernels","title":"Using the Kernels","text":""},{"location":"getting-started/#rmsnorm","title":"RMSNorm","text":"<p>RMSNorm (Root Mean Square Layer Normalization) is used in LLaMA, Mistral, Qwen, and other modern LLMs.</p> <p>Formula: <code>y = x * rsqrt(mean(x\u00b2) + \u03b5) * weight</code></p> <pre><code>import torch\nfrom rotalabs_accel import TritonRMSNorm, rmsnorm, rmsnorm_residual_fused\n\n# Module API\nnorm = TritonRMSNorm(hidden_size=4096, eps=1e-6)\nx = torch.randn(2, 512, 4096, device=\"cuda\", dtype=torch.float16)\ny = norm(x)\n\n# Functional API\nweight = torch.ones(4096, device=\"cuda\", dtype=torch.float16)\ny = rmsnorm(x, weight, eps=1e-6)\n\n# Fused with residual (common in transformer blocks)\nresidual = torch.randn_like(x)\ny = rmsnorm_residual_fused(x, residual, weight, eps=1e-6)\n# Equivalent to: rmsnorm(x + residual, weight), but faster\n</code></pre> <p>When to use <code>rmsnorm_residual_fused</code></p> <p>Transformer blocks typically add a residual before each sublayer normalization: <pre><code>x = layernorm(x + residual)  # Standard pattern\n</code></pre> The fused version eliminates an intermediate tensor allocation and memory round-trip.</p>"},{"location":"getting-started/#swiglu","title":"SwiGLU","text":"<p>SwiGLU (Swish-Gated Linear Unit) is the FFN activation used in LLaMA and PaLM.</p> <p>Formula: <code>y = SiLU(x @ W_gate) \u00d7 (x @ W_up)</code></p> <pre><code>from rotalabs_accel import SwiGLU, swiglu_fused\n\n# Full FFN module (includes projections)\nffn = SwiGLU(\n    hidden_size=4096,\n    intermediate_size=11008,  # Typically ~2.7x hidden_size\n    bias=False,\n)\nx = torch.randn(2, 512, 4096, device=\"cuda\", dtype=torch.float16)\ny = ffn(x)  # Output shape: (2, 512, 4096)\n\n# Functional API (just the activation, after you've done projections)\ngate = x @ W_gate  # Shape: (2, 512, 11008)\nup = x @ W_up      # Shape: (2, 512, 11008)\nactivated = swiglu_fused(gate, up)  # Shape: (2, 512, 11008)\noutput = activated @ W_down  # Shape: (2, 512, 4096)\n</code></pre>"},{"location":"getting-started/#rotary-position-embeddings-rope","title":"Rotary Position Embeddings (RoPE)","text":"<p>RoPE encodes position information by rotating query and key vectors.</p> <pre><code>from rotalabs_accel import RotaryEmbedding, apply_rope, build_rope_cache\n\n# Module API (manages cache automatically)\nrope = RotaryEmbedding(\n    dim=128,           # Head dimension\n    max_seq_len=8192,  # Maximum sequence length\n    base=10000.0,      # Base for frequency computation\n)\n\n# Apply to Q and K tensors\nq = torch.randn(2, 512, 32, 128, device=\"cuda\")  # [batch, seq, heads, head_dim]\nk = torch.randn(2, 512, 8, 128, device=\"cuda\")   # GQA: fewer K heads\n\nq_rot, k_rot = rope(q, k, seq_len=512)\n\n# Functional API (build cache once, apply many times)\ncos, sin = build_rope_cache(seq_len=512, head_dim=128, device=\"cuda\")\nq_rot, k_rot = apply_rope(q, k, cos, sin)\n</code></pre> <p>RoPE with different Q/K head counts</p> <p>RoPE works with Grouped Query Attention (GQA) where K/V have fewer heads than Q. The <code>apply_rope</code> function handles broadcasting automatically.</p>"},{"location":"getting-started/#int8-quantization","title":"INT8 Quantization","text":"<p>Reduce memory usage by 50% with W8A16 (INT8 weights, FP16 activations).</p> <pre><code>from rotalabs_accel import (\n    quantize_symmetric,\n    quantize_weight_per_channel,\n    dequantize,\n    calculate_quantization_error,\n    QuantizedLinear,\n    Int8Linear,\n)\n\n# Per-tensor quantization\nweight = torch.randn(4096, 4096, dtype=torch.float16)\nweight_int8, scale = quantize_symmetric(weight)\nweight_restored = dequantize(weight_int8, scale)\n\n# Per-channel quantization (better accuracy)\nweight_int8, scales = quantize_weight_per_channel(weight)\n\n# Measure quantization error\nerrors = calculate_quantization_error(weight, weight_int8, scales)\nprint(f\"Max error: {errors['max_abs_error']:.6f}\")\nprint(f\"SNR: {errors['snr_db']:.1f} dB\")\n\n# High-level: convert existing Linear layer\nlinear = torch.nn.Linear(4096, 4096)\nquantized = QuantizedLinear.from_linear(linear)\n\n# Or create directly\nqlinear = Int8Linear(in_features=4096, out_features=4096)\nqlinear.quantize_weights(pretrained_weights)\n</code></pre>"},{"location":"getting-started/#device-utilities","title":"Device Utilities","text":""},{"location":"getting-started/#automatic-device-selection","title":"Automatic Device Selection","text":"<pre><code>from rotalabs_accel import get_device\n\n# Returns best available device\ndevice = get_device()  # 'cuda' if available, else 'cpu'\n\n# Force specific device\ndevice = get_device(\"cuda:1\")\ndevice = get_device(\"cpu\")\n</code></pre>"},{"location":"getting-started/#capability-checking","title":"Capability Checking","text":"<pre><code>from rotalabs_accel import get_device_properties\n\nprops = get_device_properties()\nprint(f\"GPU: {props['name']}\")\nprint(f\"VRAM: {props['total_memory'] / 1e9:.1f} GB\")\nprint(f\"Compute capability: {props['compute_capability']}\")\nprint(f\"Supports BF16: {props['supports_bf16']}\")\nprint(f\"Supports FP8: {props['supports_fp8']}\")\n\n# Plan dtype based on capabilities\nif props['supports_bf16']:\n    dtype = torch.bfloat16\nelse:\n    dtype = torch.float16\n</code></pre>"},{"location":"getting-started/#best-practices","title":"Best Practices","text":""},{"location":"getting-started/#1-use-fp16-or-bf16-for-inference","title":"1. Use FP16 or BF16 for Inference","text":"<p>Triton kernels are optimized for 16-bit floating point:</p> <pre><code># Good - FP16 or BF16\nx = torch.randn(..., device=\"cuda\", dtype=torch.float16)\nx = torch.randn(..., device=\"cuda\", dtype=torch.bfloat16)\n\n# Works but slower - kernels will internally convert\nx = torch.randn(..., device=\"cuda\", dtype=torch.float32)\n</code></pre>"},{"location":"getting-started/#2-match-hidden-dimensions-to-powers-of-2","title":"2. Match Hidden Dimensions to Powers of 2","text":"<p>Triton kernels use block sizes that are powers of 2. Dimensions like 4096, 8192 are optimal:</p> <pre><code># Optimal\nnorm = TritonRMSNorm(4096)   # 4096 = 2^12\nnorm = TritonRMSNorm(8192)   # 8192 = 2^13\n\n# Still works but may be slightly slower\nnorm = TritonRMSNorm(4000)   # Not a power of 2\n</code></pre>"},{"location":"getting-started/#3-batch-for-throughput","title":"3. Batch for Throughput","text":"<p>The kernels are optimized for batched operations:</p> <pre><code># Better: single large batch\nx = torch.randn(32, 2048, 4096, device=\"cuda\")\ny = norm(x)\n\n# Slower: many small batches\nfor i in range(32):\n    x = torch.randn(1, 2048, 4096, device=\"cuda\")\n    y = norm(x)\n</code></pre>"},{"location":"getting-started/#4-preallocate-rope-cache","title":"4. Preallocate RoPE Cache","text":"<p>For attention with fixed max sequence length, build the cache once:</p> <pre><code># At initialization\ncos, sin = build_rope_cache(max_seq_len, head_dim, device=\"cuda\")\n\n# During inference\nq_rot, k_rot = apply_rope(q, k, cos[:seq_len], sin[:seq_len])\n</code></pre>"},{"location":"getting-started/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/#triton-not-found","title":"Triton Not Found","text":"<pre><code>ImportError: No module named 'triton'\n</code></pre> <p>Solution: Install with Triton extras: <code>pip install rotalabs-accel[triton]</code></p> <p>Note: Triton only works on Linux with NVIDIA GPUs. On other platforms, PyTorch fallbacks are used automatically.</p>"},{"location":"getting-started/#cuda-out-of-memory","title":"CUDA Out of Memory","text":"<p>If you're hitting OOM with large models, use INT8 quantization:</p> <pre><code># Before: 32-bit weights (full precision)\nlinear = torch.nn.Linear(4096, 4096).cuda()  # 67 MB\n\n# After: 8-bit weights\nqlinear = QuantizedLinear.from_linear(linear)  # 16 MB + overhead\n</code></pre>"},{"location":"getting-started/#numerical-differences","title":"Numerical Differences","text":"<p>Triton kernels use FP32 accumulation for numerical stability, but minor differences from PyTorch are expected:</p> <pre><code># Maximum expected difference (should be &lt; 1e-3 for FP16)\ntorch.allclose(triton_output, pytorch_output, atol=1e-3, rtol=1e-3)\n</code></pre>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<ul> <li>API Reference - Complete API documentation</li> <li>Kernel Details - Deep dive into each kernel</li> <li>Quantization Guide - Advanced quantization techniques</li> </ul>"},{"location":"api/","title":"API Reference","text":"<p>Complete API documentation for rotalabs-accel, auto-generated from source code docstrings.</p>"},{"location":"api/#package-overview","title":"Package Overview","text":"<pre><code>import rotalabs_accel\n\n# Version\nrotalabs_accel.__version__  # e.g., \"0.1.0\"\n\n# All public exports\nrotalabs_accel.__all__\n</code></pre>"},{"location":"api/#architecture","title":"Architecture","text":"<pre><code>rotalabs_accel/\n\u251c\u2500\u2500 kernels/                    # Triton-optimized kernels\n\u2502   \u251c\u2500\u2500 normalization.py        # RMSNorm, rmsnorm_residual_fused\n\u2502   \u251c\u2500\u2500 activations.py          # SwiGLU, swiglu_fused\n\u2502   \u251c\u2500\u2500 rope.py                 # RoPE, apply_rope, build_rope_cache\n\u2502   \u2514\u2500\u2500 gemm.py                 # INT8 GEMM, Int8Linear\n\u251c\u2500\u2500 quantization/\n\u2502   \u2514\u2500\u2500 symmetric.py            # INT8 symmetric quantization utilities\n\u2514\u2500\u2500 utils/\n    \u2514\u2500\u2500 device.py               # Device detection and capability checking\n</code></pre>"},{"location":"api/#quick-reference","title":"Quick Reference","text":""},{"location":"api/#normalization","title":"Normalization","text":"Export Type Description <code>TritonRMSNorm</code> <code>nn.Module</code> Drop-in RMSNorm replacement <code>rmsnorm</code> Function Apply RMS normalization <code>rmsnorm_torch</code> Function PyTorch-only reference implementation <code>rmsnorm_residual_fused</code> Function Fused RMSNorm + residual addition"},{"location":"api/#activations","title":"Activations","text":"Export Type Description <code>SwiGLU</code> <code>nn.Module</code> Full SwiGLU FFN with projections <code>swiglu_fused</code> Function Fused SiLU(gate) \u00d7 up activation <code>swiglu_torch</code> Function PyTorch-only reference implementation"},{"location":"api/#position-embeddings","title":"Position Embeddings","text":"Export Type Description <code>RotaryEmbedding</code> <code>nn.Module</code> Self-contained RoPE module <code>apply_rope</code> Function Apply rotary embeddings to Q/K <code>rope_torch</code> Function PyTorch-only reference implementation <code>build_rope_cache</code> Function Precompute cos/sin tables"},{"location":"api/#matrix-multiplication","title":"Matrix Multiplication","text":"Export Type Description <code>Int8Linear</code> <code>nn.Module</code> INT8 quantized linear layer <code>int8_gemm</code> Function W8A16 matrix multiplication <code>int8_gemm_torch</code> Function PyTorch-only reference implementation"},{"location":"api/#quantization-utilities","title":"Quantization Utilities","text":"Export Type Description <code>QuantizedLinear</code> <code>nn.Module</code> Higher-level quantized linear layer <code>quantize_symmetric</code> Function Symmetric INT8 quantization <code>dequantize</code> Function INT8 \u2192 FP16/FP32 conversion <code>quantize_weight_per_channel</code> Function Per-channel weight quantization <code>calculate_quantization_error</code> Function Measure quantization accuracy"},{"location":"api/#device-utilities","title":"Device Utilities","text":"Export Type Description <code>get_device</code> Function Auto-detect best available device <code>is_cuda_available</code> Function Check for CUDA support <code>is_triton_available</code> Function Check for Triton installation <code>get_device_properties</code> Function Get GPU capabilities"},{"location":"api/#import-patterns","title":"Import Patterns","text":""},{"location":"api/#recommended-import-specific-functionsclasses","title":"Recommended: Import specific functions/classes","text":"<pre><code>from rotalabs_accel import (\n    TritonRMSNorm,\n    SwiGLU,\n    RotaryEmbedding,\n    get_device,\n    is_triton_available,\n)\n</code></pre>"},{"location":"api/#alternative-import-from-submodules","title":"Alternative: Import from submodules","text":"<pre><code>from rotalabs_accel.kernels import rmsnorm, swiglu_fused, apply_rope\nfrom rotalabs_accel.quantization import quantize_symmetric, dequantize\nfrom rotalabs_accel.utils import get_device, get_device_properties\n</code></pre>"},{"location":"api/#for-type-checking","title":"For type checking","text":"<pre><code>from typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from rotalabs_accel import TritonRMSNorm, SwiGLU\n</code></pre>"},{"location":"api/#sections","title":"Sections","text":"<ul> <li> <p>:material-function: Kernels</p> <p>Triton-optimized implementations of core LLM operations.</p> <ul> <li>Normalization - RMSNorm</li> <li>Activations - SwiGLU</li> <li>RoPE - Rotary Position Embeddings</li> <li>GEMM - INT8 Matrix Multiplication</li> </ul> </li> <li> <p>:material-memory: Quantization</p> <p>INT8 symmetric quantization for memory-efficient inference.</p> <p>Quantization \u2192</p> </li> <li> <p>:material-view-module: Modules</p> <p>Drop-in <code>nn.Module</code> replacements.</p> <p>Modules \u2192</p> </li> <li> <p>:material-tools: Utilities</p> <p>Device detection and capability checking.</p> <p>Utilities \u2192</p> </li> </ul>"},{"location":"api/modules/","title":"Modules","text":"<p>Drop-in <code>nn.Module</code> replacements with optimized Triton kernels.</p>"},{"location":"api/modules/#overview","title":"Overview","text":"<p>These modules provide the same interface as their PyTorch counterparts but use optimized Triton kernels when available. If Triton isn't installed or the input is on CPU, they automatically fall back to pure PyTorch.</p> <pre><code># These work identically, but TritonRMSNorm is faster on GPU\nnorm_pytorch = torch.nn.RMSNorm(4096)\nnorm_triton = TritonRMSNorm(4096)\n\n# Same API, same results, different speed\ny1 = norm_pytorch(x)\ny2 = norm_triton(x)  # Up to 3.8x faster on GPU\n</code></pre>"},{"location":"api/modules/#module-summary","title":"Module Summary","text":"Module Replaces Speedup Use Case <code>TritonRMSNorm</code> <code>nn.RMSNorm</code> 3.8x LLaMA, Mistral normalization <code>SwiGLU</code> Custom FFN 2.9x LLaMA, PaLM FFN layers <code>RotaryEmbedding</code> Manual RoPE 2.9x Position encoding <code>Int8Linear</code> <code>nn.Linear</code> 3.3x Memory-efficient inference <code>QuantizedLinear</code> <code>nn.Linear</code> 3.3x Easy model quantization"},{"location":"api/modules/#tritonrmsnorm","title":"TritonRMSNorm","text":"<p>RMS normalization layer, used in LLaMA, Mistral, Qwen.</p> <pre><code>from rotalabs_accel import TritonRMSNorm\n\nnorm = TritonRMSNorm(hidden_size=4096, eps=1e-6)\nx = torch.randn(2, 512, 4096, device=\"cuda\", dtype=torch.float16)\ny = norm(x)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>RMSNorm layer with automatic Triton/PyTorch dispatch.</p> <p>Drop-in replacement for torch.nn.RMSNorm with identical interface. Uses Triton kernel on CUDA when available, otherwise falls back to PyTorch.</p> <p>Parameters:</p> Name Type Description Default <code>hidden_size</code> <code>int</code> <p>Size of the last dimension to normalize over.</p> required <code>eps</code> <code>float</code> <p>Small constant for numerical stability.</p> <code>1e-06</code> Example <p>norm = TritonRMSNorm(64) x = torch.randn(2, 8, 64) y = norm(x)</p> Source code in <code>src/rotalabs_accel/kernels/normalization.py</code> <pre><code>class TritonRMSNorm(torch.nn.Module):\n    \"\"\"\n    RMSNorm layer with automatic Triton/PyTorch dispatch.\n\n    Drop-in replacement for torch.nn.RMSNorm with identical interface.\n    Uses Triton kernel on CUDA when available, otherwise falls back to PyTorch.\n\n    Args:\n        hidden_size: Size of the last dimension to normalize over.\n        eps: Small constant for numerical stability.\n\n    Example:\n        &gt;&gt;&gt; norm = TritonRMSNorm(64)\n        &gt;&gt;&gt; x = torch.randn(2, 8, 64)\n        &gt;&gt;&gt; y = norm(x)\n    \"\"\"\n\n    def __init__(self, hidden_size: int, eps: float = 1e-6):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.eps = eps\n        self.weight = torch.nn.Parameter(torch.ones(hidden_size))\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        return rmsnorm(x, self.weight, self.eps)\n\n    def extra_repr(self) -&gt; str:\n        return f\"{self.hidden_size}, eps={self.eps}\"\n</code></pre>"},{"location":"api/modules/#rotalabs_accel.kernels.normalization.TritonRMSNorm.__init__","title":"__init__","text":"<pre><code>__init__(hidden_size: int, eps: float = 1e-06)\n</code></pre> Source code in <code>src/rotalabs_accel/kernels/normalization.py</code> <pre><code>def __init__(self, hidden_size: int, eps: float = 1e-6):\n    super().__init__()\n    self.hidden_size = hidden_size\n    self.eps = eps\n    self.weight = torch.nn.Parameter(torch.ones(hidden_size))\n</code></pre>"},{"location":"api/modules/#rotalabs_accel.kernels.normalization.TritonRMSNorm.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; torch.Tensor\n</code></pre> Source code in <code>src/rotalabs_accel/kernels/normalization.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    return rmsnorm(x, self.weight, self.eps)\n</code></pre>"},{"location":"api/modules/#swiglu","title":"SwiGLU","text":"<p>Complete SwiGLU FFN module with gate, up, and down projections.</p> <pre><code>from rotalabs_accel import SwiGLU\n\nffn = SwiGLU(\n    hidden_size=4096,\n    intermediate_size=11008,\n    bias=False,\n)\nx = torch.randn(2, 512, 4096, device=\"cuda\", dtype=torch.float16)\ny = ffn(x)  # Shape: (2, 512, 4096)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>SwiGLU module with linear projections.</p> Implements the full SwiGLU FFN <p>y = (silu(x @ W_gate) * (x @ W_up)) @ W_down</p> <p>Uses Triton kernel on CUDA when available, otherwise falls back to PyTorch.</p> <p>Parameters:</p> Name Type Description Default <code>hidden_size</code> <code>int</code> <p>Input/output dimension.</p> required <code>intermediate_size</code> <code>int</code> <p>Intermediate dimension for the FFN.</p> required <code>bias</code> <code>bool</code> <p>Whether to use bias in linear layers.</p> <code>False</code> Example <p>swiglu = SwiGLU(hidden_size=64, intermediate_size=256) x = torch.randn(2, 8, 64) y = swiglu(x)  # Shape: (2, 8, 64)</p> Source code in <code>src/rotalabs_accel/kernels/activations.py</code> <pre><code>class SwiGLU(torch.nn.Module):\n    \"\"\"\n    SwiGLU module with linear projections.\n\n    Implements the full SwiGLU FFN:\n        y = (silu(x @ W_gate) * (x @ W_up)) @ W_down\n\n    Uses Triton kernel on CUDA when available, otherwise falls back to PyTorch.\n\n    Args:\n        hidden_size: Input/output dimension.\n        intermediate_size: Intermediate dimension for the FFN.\n        bias: Whether to use bias in linear layers.\n\n    Example:\n        &gt;&gt;&gt; swiglu = SwiGLU(hidden_size=64, intermediate_size=256)\n        &gt;&gt;&gt; x = torch.randn(2, 8, 64)\n        &gt;&gt;&gt; y = swiglu(x)  # Shape: (2, 8, 64)\n    \"\"\"\n\n    def __init__(\n        self,\n        hidden_size: int,\n        intermediate_size: int,\n        bias: bool = False,\n    ):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.intermediate_size = intermediate_size\n\n        self.w_gate = torch.nn.Linear(hidden_size, intermediate_size, bias=bias)\n        self.w_up = torch.nn.Linear(hidden_size, intermediate_size, bias=bias)\n        self.w_down = torch.nn.Linear(intermediate_size, hidden_size, bias=bias)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        gate = self.w_gate(x)\n        up = self.w_up(x)\n        return self.w_down(swiglu_fused(gate, up))\n\n    def extra_repr(self) -&gt; str:\n        return f\"hidden_size={self.hidden_size}, intermediate_size={self.intermediate_size}\"\n</code></pre>"},{"location":"api/modules/#rotalabs_accel.kernels.activations.SwiGLU.__init__","title":"__init__","text":"<pre><code>__init__(\n    hidden_size: int,\n    intermediate_size: int,\n    bias: bool = False,\n)\n</code></pre> Source code in <code>src/rotalabs_accel/kernels/activations.py</code> <pre><code>def __init__(\n    self,\n    hidden_size: int,\n    intermediate_size: int,\n    bias: bool = False,\n):\n    super().__init__()\n    self.hidden_size = hidden_size\n    self.intermediate_size = intermediate_size\n\n    self.w_gate = torch.nn.Linear(hidden_size, intermediate_size, bias=bias)\n    self.w_up = torch.nn.Linear(hidden_size, intermediate_size, bias=bias)\n    self.w_down = torch.nn.Linear(intermediate_size, hidden_size, bias=bias)\n</code></pre>"},{"location":"api/modules/#rotalabs_accel.kernels.activations.SwiGLU.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; torch.Tensor\n</code></pre> Source code in <code>src/rotalabs_accel/kernels/activations.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    gate = self.w_gate(x)\n    up = self.w_up(x)\n    return self.w_down(swiglu_fused(gate, up))\n</code></pre>"},{"location":"api/modules/#rotaryembedding","title":"RotaryEmbedding","text":"<p>Rotary Position Embeddings with automatic cache management.</p> <pre><code>from rotalabs_accel import RotaryEmbedding\n\nrope = RotaryEmbedding(\n    dim=128,\n    max_seq_len=8192,\n    base=10000.0,\n)\n\n# Apply to query and key\nq = torch.randn(2, 512, 32, 128, device=\"cuda\")\nk = torch.randn(2, 512, 32, 128, device=\"cuda\")\nq_rot, k_rot = rope(q, k, seq_len=512)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Rotary Position Embedding module.</p> <p>Uses Triton kernel on CUDA when available, otherwise falls back to PyTorch.</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>Dimension of each attention head (head_dim).</p> required <code>max_seq_len</code> <code>int</code> <p>Maximum sequence length to cache.</p> <code>2048</code> <code>base</code> <code>float</code> <p>Base for frequency computation (default: 10000).</p> <code>10000.0</code> Example <p>rope = RotaryEmbedding(dim=32, max_seq_len=128) q = torch.randn(2, 16, 4, 32) k = torch.randn(2, 16, 4, 32) q_rot, k_rot = rope(q, k)</p> Source code in <code>src/rotalabs_accel/kernels/rope.py</code> <pre><code>class RotaryEmbedding(torch.nn.Module):\n    \"\"\"\n    Rotary Position Embedding module.\n\n    Uses Triton kernel on CUDA when available, otherwise falls back to PyTorch.\n\n    Args:\n        dim: Dimension of each attention head (head_dim).\n        max_seq_len: Maximum sequence length to cache.\n        base: Base for frequency computation (default: 10000).\n\n    Example:\n        &gt;&gt;&gt; rope = RotaryEmbedding(dim=32, max_seq_len=128)\n        &gt;&gt;&gt; q = torch.randn(2, 16, 4, 32)\n        &gt;&gt;&gt; k = torch.randn(2, 16, 4, 32)\n        &gt;&gt;&gt; q_rot, k_rot = rope(q, k)\n    \"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        max_seq_len: int = 2048,\n        base: float = 10000.0,\n    ):\n        super().__init__()\n        self.dim = dim\n        self.max_seq_len = max_seq_len\n        self.base = base\n\n        # Build initial cache\n        cos, sin = build_rope_cache(max_seq_len, dim, base)\n        self.register_buffer(\"cos_cache\", cos, persistent=False)\n        self.register_buffer(\"sin_cache\", sin, persistent=False)\n\n    def _extend_cache(self, seq_len: int) -&gt; None:\n        \"\"\"Extend cache if sequence is longer than current cache.\"\"\"\n        if seq_len &lt;= self.cos_cache.shape[0]:\n            return\n\n        new_len = max(seq_len, self.cos_cache.shape[0] * 2)\n        cos, sin = build_rope_cache(\n            new_len,\n            self.dim,\n            self.base,\n            device=self.cos_cache.device,\n            dtype=self.cos_cache.dtype,\n        )\n        self.cos_cache = cos\n        self.sin_cache = sin\n\n    def forward(\n        self,\n        q: torch.Tensor,\n        k: torch.Tensor,\n        position_ids: Optional[torch.Tensor] = None,\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Apply RoPE to query and key tensors.\n\n        Args:\n            q: Query tensor [batch, seq, heads, head_dim]\n            k: Key tensor [batch, seq, heads, head_dim]\n            position_ids: Optional position indices [batch, seq].\n\n        Returns:\n            Tuple of (q_rotated, k_rotated).\n        \"\"\"\n        seq_len = q.shape[1]\n        self._extend_cache(seq_len)\n\n        if position_ids is None:\n            cos = self.cos_cache[:seq_len]\n            sin = self.sin_cache[:seq_len]\n        else:\n            cos = self.cos_cache[position_ids]\n            sin = self.sin_cache[position_ids]\n\n        return apply_rope(q, k, cos, sin)\n\n    def extra_repr(self) -&gt; str:\n        return f\"dim={self.dim}, max_seq_len={self.max_seq_len}, base={self.base}\"\n</code></pre>"},{"location":"api/modules/#rotalabs_accel.kernels.rope.RotaryEmbedding.__init__","title":"__init__","text":"<pre><code>__init__(\n    dim: int, max_seq_len: int = 2048, base: float = 10000.0\n)\n</code></pre> Source code in <code>src/rotalabs_accel/kernels/rope.py</code> <pre><code>def __init__(\n    self,\n    dim: int,\n    max_seq_len: int = 2048,\n    base: float = 10000.0,\n):\n    super().__init__()\n    self.dim = dim\n    self.max_seq_len = max_seq_len\n    self.base = base\n\n    # Build initial cache\n    cos, sin = build_rope_cache(max_seq_len, dim, base)\n    self.register_buffer(\"cos_cache\", cos, persistent=False)\n    self.register_buffer(\"sin_cache\", sin, persistent=False)\n</code></pre>"},{"location":"api/modules/#rotalabs_accel.kernels.rope.RotaryEmbedding.forward","title":"forward","text":"<pre><code>forward(\n    q: Tensor,\n    k: Tensor,\n    position_ids: Optional[Tensor] = None,\n) -&gt; Tuple[torch.Tensor, torch.Tensor]\n</code></pre> <p>Apply RoPE to query and key tensors.</p> <p>Parameters:</p> Name Type Description Default <code>q</code> <code>Tensor</code> <p>Query tensor [batch, seq, heads, head_dim]</p> required <code>k</code> <code>Tensor</code> <p>Key tensor [batch, seq, heads, head_dim]</p> required <code>position_ids</code> <code>Optional[Tensor]</code> <p>Optional position indices [batch, seq].</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>Tuple of (q_rotated, k_rotated).</p> Source code in <code>src/rotalabs_accel/kernels/rope.py</code> <pre><code>def forward(\n    self,\n    q: torch.Tensor,\n    k: torch.Tensor,\n    position_ids: Optional[torch.Tensor] = None,\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Apply RoPE to query and key tensors.\n\n    Args:\n        q: Query tensor [batch, seq, heads, head_dim]\n        k: Key tensor [batch, seq, heads, head_dim]\n        position_ids: Optional position indices [batch, seq].\n\n    Returns:\n        Tuple of (q_rotated, k_rotated).\n    \"\"\"\n    seq_len = q.shape[1]\n    self._extend_cache(seq_len)\n\n    if position_ids is None:\n        cos = self.cos_cache[:seq_len]\n        sin = self.sin_cache[:seq_len]\n    else:\n        cos = self.cos_cache[position_ids]\n        sin = self.sin_cache[position_ids]\n\n    return apply_rope(q, k, cos, sin)\n</code></pre>"},{"location":"api/modules/#int8linear","title":"Int8Linear","text":"<p>Linear layer with INT8 quantized weights.</p> <pre><code>from rotalabs_accel import Int8Linear\n\nlinear = Int8Linear(\n    in_features=4096,\n    out_features=4096,\n    bias=False,\n)\nlinear.quantize_weights(pretrained_weight)\ny = linear(x)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Linear layer using INT8 weights with optimized GEMM kernel.</p> <p>Uses Triton kernel on CUDA when available, otherwise falls back to PyTorch.</p> <p>Parameters:</p> Name Type Description Default <code>in_features</code> <code>int</code> <p>Input dimension (K).</p> required <code>out_features</code> <code>int</code> <p>Output dimension (N).</p> required <code>bias</code> <code>bool</code> <p>Whether to include bias.</p> <code>False</code> Example <p>linear = Int8Linear(64, 128) linear.quantize_weights(torch.randn(128, 64)) y = linear(torch.randn(2, 8, 64))</p> Source code in <code>src/rotalabs_accel/kernels/gemm.py</code> <pre><code>class Int8Linear(torch.nn.Module):\n    \"\"\"\n    Linear layer using INT8 weights with optimized GEMM kernel.\n\n    Uses Triton kernel on CUDA when available, otherwise falls back to PyTorch.\n\n    Args:\n        in_features: Input dimension (K).\n        out_features: Output dimension (N).\n        bias: Whether to include bias.\n\n    Example:\n        &gt;&gt;&gt; linear = Int8Linear(64, 128)\n        &gt;&gt;&gt; linear.quantize_weights(torch.randn(128, 64))\n        &gt;&gt;&gt; y = linear(torch.randn(2, 8, 64))\n    \"\"\"\n\n    def __init__(\n        self,\n        in_features: int,\n        out_features: int,\n        bias: bool = False,\n    ):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n\n        # INT8 weights: (out_features, in_features)\n        self.register_buffer(\n            \"weight_int8\",\n            torch.zeros(out_features, in_features, dtype=torch.int8)\n        )\n        self.register_buffer(\n            \"scale\",\n            torch.ones(out_features, dtype=torch.float32)\n        )\n        # Pre-transposed weight for Triton path\n        self.register_buffer(\n            \"weight_transposed\",\n            torch.zeros(in_features, out_features, dtype=torch.int8)\n        )\n\n        if bias:\n            self.bias = torch.nn.Parameter(torch.zeros(out_features, dtype=torch.float16))\n        else:\n            self.register_parameter(\"bias\", None)\n\n        self._quantized = False\n\n    def quantize_weights(self, weight: torch.Tensor) -&gt; None:\n        \"\"\"Quantize and store FP16/FP32 weights as INT8.\"\"\"\n        assert weight.shape == (self.out_features, self.in_features)\n\n        # Quantization happens on CPU\n        weight_cpu = weight.cpu() if weight.is_cuda else weight\n        weight_int8, scale = quantize_weight_per_channel(weight_cpu)\n\n        # Move to same device as buffers and copy\n        device = self.weight_int8.device\n        self.weight_int8.copy_(weight_int8.to(device))\n        self.scale.copy_(scale.to(device))\n        self.weight_transposed.copy_(weight_int8.t().contiguous().to(device))\n        self._quantized = True\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass using optimized INT8 GEMM kernel.\"\"\"\n        if not self._quantized:\n            raise RuntimeError(\"Weights not quantized. Call quantize_weights() first.\")\n\n        return int8_gemm(\n            x, self.weight_int8, self.scale, self.bias,\n            weight_transposed=self.weight_transposed,\n        )\n\n    @classmethod\n    def from_linear(cls, linear: torch.nn.Linear) -&gt; \"Int8Linear\":\n        \"\"\"Convert nn.Linear to Int8Linear.\"\"\"\n        has_bias = linear.bias is not None\n        int8_linear = cls(linear.in_features, linear.out_features, bias=has_bias)\n\n        # Move to same device as input linear, then quantize\n        device = linear.weight.device\n        int8_linear = int8_linear.to(device)\n        int8_linear.quantize_weights(linear.weight.data)\n\n        # Copy bias if present\n        if has_bias:\n            int8_linear.bias.data.copy_(linear.bias.data.half())\n\n        return int8_linear\n\n    def extra_repr(self) -&gt; str:\n        return (\n            f\"in_features={self.in_features}, out_features={self.out_features}, \"\n            f\"bias={self.bias is not None}, quantized={self._quantized}\"\n        )\n</code></pre>"},{"location":"api/modules/#rotalabs_accel.kernels.gemm.Int8Linear.__init__","title":"__init__","text":"<pre><code>__init__(\n    in_features: int, out_features: int, bias: bool = False\n)\n</code></pre> Source code in <code>src/rotalabs_accel/kernels/gemm.py</code> <pre><code>def __init__(\n    self,\n    in_features: int,\n    out_features: int,\n    bias: bool = False,\n):\n    super().__init__()\n    self.in_features = in_features\n    self.out_features = out_features\n\n    # INT8 weights: (out_features, in_features)\n    self.register_buffer(\n        \"weight_int8\",\n        torch.zeros(out_features, in_features, dtype=torch.int8)\n    )\n    self.register_buffer(\n        \"scale\",\n        torch.ones(out_features, dtype=torch.float32)\n    )\n    # Pre-transposed weight for Triton path\n    self.register_buffer(\n        \"weight_transposed\",\n        torch.zeros(in_features, out_features, dtype=torch.int8)\n    )\n\n    if bias:\n        self.bias = torch.nn.Parameter(torch.zeros(out_features, dtype=torch.float16))\n    else:\n        self.register_parameter(\"bias\", None)\n\n    self._quantized = False\n</code></pre>"},{"location":"api/modules/#rotalabs_accel.kernels.gemm.Int8Linear.quantize_weights","title":"quantize_weights","text":"<pre><code>quantize_weights(weight: Tensor) -&gt; None\n</code></pre> <p>Quantize and store FP16/FP32 weights as INT8.</p> Source code in <code>src/rotalabs_accel/kernels/gemm.py</code> <pre><code>def quantize_weights(self, weight: torch.Tensor) -&gt; None:\n    \"\"\"Quantize and store FP16/FP32 weights as INT8.\"\"\"\n    assert weight.shape == (self.out_features, self.in_features)\n\n    # Quantization happens on CPU\n    weight_cpu = weight.cpu() if weight.is_cuda else weight\n    weight_int8, scale = quantize_weight_per_channel(weight_cpu)\n\n    # Move to same device as buffers and copy\n    device = self.weight_int8.device\n    self.weight_int8.copy_(weight_int8.to(device))\n    self.scale.copy_(scale.to(device))\n    self.weight_transposed.copy_(weight_int8.t().contiguous().to(device))\n    self._quantized = True\n</code></pre>"},{"location":"api/modules/#rotalabs_accel.kernels.gemm.Int8Linear.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Forward pass using optimized INT8 GEMM kernel.</p> Source code in <code>src/rotalabs_accel/kernels/gemm.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass using optimized INT8 GEMM kernel.\"\"\"\n    if not self._quantized:\n        raise RuntimeError(\"Weights not quantized. Call quantize_weights() first.\")\n\n    return int8_gemm(\n        x, self.weight_int8, self.scale, self.bias,\n        weight_transposed=self.weight_transposed,\n    )\n</code></pre>"},{"location":"api/modules/#quantizedlinear","title":"QuantizedLinear","text":"<p>Higher-level quantized linear with easy conversion from <code>nn.Linear</code>.</p> <pre><code>from rotalabs_accel import QuantizedLinear\n\n# Convert existing layer\nlinear = torch.nn.Linear(4096, 4096)\nqlinear = QuantizedLinear.from_linear(linear)\n\n# Use like normal\ny = qlinear(x)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Linear layer with INT8 quantized weights.</p> <p>Stores weights in INT8 format and dequantizes on-the-fly during forward pass. This is a reference implementation - the actual kernel-level optimization happens in the Triton INT8 GEMM kernel.</p> <p>For W8A16 inference: - Weights: INT8 (2x memory reduction) - Activations: FP16 - Compute: FP16 with FP32 accumulation</p> <p>Parameters:</p> Name Type Description Default <code>in_features</code> <code>int</code> <p>Input dimension.</p> required <code>out_features</code> <code>int</code> <p>Output dimension.</p> required <code>bias</code> <code>bool</code> <p>Whether to include bias (default: False for LLM weights).</p> <code>False</code> Example <p>linear = QuantizedLinear(4096, 4096) linear.quantize_weights(pretrained_weight) y = linear(x)  # x is FP16, y is FP16</p> Source code in <code>src/rotalabs_accel/quantization/symmetric.py</code> <pre><code>class QuantizedLinear(torch.nn.Module):\n    \"\"\"\n    Linear layer with INT8 quantized weights.\n\n    Stores weights in INT8 format and dequantizes on-the-fly during forward pass.\n    This is a reference implementation - the actual kernel-level optimization\n    happens in the Triton INT8 GEMM kernel.\n\n    For W8A16 inference:\n    - Weights: INT8 (2x memory reduction)\n    - Activations: FP16\n    - Compute: FP16 with FP32 accumulation\n\n    Args:\n        in_features: Input dimension.\n        out_features: Output dimension.\n        bias: Whether to include bias (default: False for LLM weights).\n\n    Example:\n        &gt;&gt;&gt; linear = QuantizedLinear(4096, 4096)\n        &gt;&gt;&gt; linear.quantize_weights(pretrained_weight)\n        &gt;&gt;&gt; y = linear(x)  # x is FP16, y is FP16\n    \"\"\"\n\n    def __init__(\n        self,\n        in_features: int,\n        out_features: int,\n        bias: bool = False,\n    ):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n\n        # Register buffers for quantized weights\n        self.register_buffer(\n            \"weight_int8\",\n            torch.zeros(out_features, in_features, dtype=torch.int8)\n        )\n        self.register_buffer(\n            \"weight_scale\",\n            torch.ones(out_features, dtype=torch.float32)\n        )\n\n        if bias:\n            self.bias = torch.nn.Parameter(torch.zeros(out_features))\n        else:\n            self.register_parameter(\"bias\", None)\n\n        self._quantized = False\n\n    def quantize_weights(self, weight: torch.Tensor) -&gt; None:\n        \"\"\"\n        Quantize and store weights.\n\n        Args:\n            weight: FP16 or FP32 weight tensor of shape (out_features, in_features).\n        \"\"\"\n        assert weight.shape == (self.out_features, self.in_features)\n        weight_int8, scale = quantize_weight_per_channel(weight)\n        self.weight_int8.copy_(weight_int8)\n        self.weight_scale.copy_(scale)\n        self._quantized = True\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass with on-the-fly dequantization.\n\n        Args:\n            x: Input tensor of shape (..., in_features).\n\n        Returns:\n            Output tensor of shape (..., out_features).\n        \"\"\"\n        if not self._quantized:\n            raise RuntimeError(\"Weights not quantized. Call quantize_weights() first.\")\n\n        # Dequantize weights\n        weight = dequantize(self.weight_int8, self.weight_scale, dtype=x.dtype, dim=0)\n\n        # Compute matmul\n        y = torch.nn.functional.linear(x, weight, self.bias)\n\n        return y\n\n    @classmethod\n    def from_linear(cls, linear: torch.nn.Linear) -&gt; \"QuantizedLinear\":\n        \"\"\"\n        Create QuantizedLinear from existing nn.Linear.\n\n        Args:\n            linear: Pretrained linear layer.\n\n        Returns:\n            QuantizedLinear with quantized weights.\n        \"\"\"\n        has_bias = linear.bias is not None\n        quantized = cls(linear.in_features, linear.out_features, bias=has_bias)\n        quantized.quantize_weights(linear.weight.data)\n        if has_bias:\n            quantized.bias.data.copy_(linear.bias.data)\n        return quantized\n\n    def extra_repr(self) -&gt; str:\n        return (\n            f\"in_features={self.in_features}, out_features={self.out_features}, \"\n            f\"bias={self.bias is not None}, quantized={self._quantized}\"\n        )\n</code></pre>"},{"location":"api/modules/#rotalabs_accel.quantization.symmetric.QuantizedLinear.__init__","title":"__init__","text":"<pre><code>__init__(\n    in_features: int, out_features: int, bias: bool = False\n)\n</code></pre> Source code in <code>src/rotalabs_accel/quantization/symmetric.py</code> <pre><code>def __init__(\n    self,\n    in_features: int,\n    out_features: int,\n    bias: bool = False,\n):\n    super().__init__()\n    self.in_features = in_features\n    self.out_features = out_features\n\n    # Register buffers for quantized weights\n    self.register_buffer(\n        \"weight_int8\",\n        torch.zeros(out_features, in_features, dtype=torch.int8)\n    )\n    self.register_buffer(\n        \"weight_scale\",\n        torch.ones(out_features, dtype=torch.float32)\n    )\n\n    if bias:\n        self.bias = torch.nn.Parameter(torch.zeros(out_features))\n    else:\n        self.register_parameter(\"bias\", None)\n\n    self._quantized = False\n</code></pre>"},{"location":"api/modules/#rotalabs_accel.quantization.symmetric.QuantizedLinear.quantize_weights","title":"quantize_weights","text":"<pre><code>quantize_weights(weight: Tensor) -&gt; None\n</code></pre> <p>Quantize and store weights.</p> <p>Parameters:</p> Name Type Description Default <code>weight</code> <code>Tensor</code> <p>FP16 or FP32 weight tensor of shape (out_features, in_features).</p> required Source code in <code>src/rotalabs_accel/quantization/symmetric.py</code> <pre><code>def quantize_weights(self, weight: torch.Tensor) -&gt; None:\n    \"\"\"\n    Quantize and store weights.\n\n    Args:\n        weight: FP16 or FP32 weight tensor of shape (out_features, in_features).\n    \"\"\"\n    assert weight.shape == (self.out_features, self.in_features)\n    weight_int8, scale = quantize_weight_per_channel(weight)\n    self.weight_int8.copy_(weight_int8)\n    self.weight_scale.copy_(scale)\n    self._quantized = True\n</code></pre>"},{"location":"api/modules/#rotalabs_accel.quantization.symmetric.QuantizedLinear.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Forward pass with on-the-fly dequantization.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (..., in_features).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor of shape (..., out_features).</p> Source code in <code>src/rotalabs_accel/quantization/symmetric.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass with on-the-fly dequantization.\n\n    Args:\n        x: Input tensor of shape (..., in_features).\n\n    Returns:\n        Output tensor of shape (..., out_features).\n    \"\"\"\n    if not self._quantized:\n        raise RuntimeError(\"Weights not quantized. Call quantize_weights() first.\")\n\n    # Dequantize weights\n    weight = dequantize(self.weight_int8, self.weight_scale, dtype=x.dtype, dim=0)\n\n    # Compute matmul\n    y = torch.nn.functional.linear(x, weight, self.bias)\n\n    return y\n</code></pre>"},{"location":"api/modules/#rotalabs_accel.quantization.symmetric.QuantizedLinear.from_linear","title":"from_linear  <code>classmethod</code>","text":"<pre><code>from_linear(linear: Linear) -&gt; QuantizedLinear\n</code></pre> <p>Create QuantizedLinear from existing nn.Linear.</p> <p>Parameters:</p> Name Type Description Default <code>linear</code> <code>Linear</code> <p>Pretrained linear layer.</p> required <p>Returns:</p> Type Description <code>QuantizedLinear</code> <p>QuantizedLinear with quantized weights.</p> Source code in <code>src/rotalabs_accel/quantization/symmetric.py</code> <pre><code>@classmethod\ndef from_linear(cls, linear: torch.nn.Linear) -&gt; \"QuantizedLinear\":\n    \"\"\"\n    Create QuantizedLinear from existing nn.Linear.\n\n    Args:\n        linear: Pretrained linear layer.\n\n    Returns:\n        QuantizedLinear with quantized weights.\n    \"\"\"\n    has_bias = linear.bias is not None\n    quantized = cls(linear.in_features, linear.out_features, bias=has_bias)\n    quantized.quantize_weights(linear.weight.data)\n    if has_bias:\n        quantized.bias.data.copy_(linear.bias.data)\n    return quantized\n</code></pre>"},{"location":"api/modules/#using-with-existing-models","title":"Using with Existing Models","text":""},{"location":"api/modules/#replace-layers-in-a-model","title":"Replace Layers in a Model","text":"<pre><code>from rotalabs_accel import TritonRMSNorm, SwiGLU, QuantizedLinear\n\ndef optimize_model(model):\n    \"\"\"Replace layers with optimized versions.\"\"\"\n    for name, module in model.named_children():\n        # Replace RMSNorm\n        if isinstance(module, torch.nn.RMSNorm):\n            setattr(model, name, TritonRMSNorm(module.weight.shape[0]))\n\n        # Quantize Linear\n        elif isinstance(module, torch.nn.Linear):\n            setattr(model, name, QuantizedLinear.from_linear(module))\n\n        # Recurse\n        else:\n            optimize_model(module)\n\n    return model\n</code></pre>"},{"location":"api/modules/#with-hugging-face-transformers","title":"With Hugging Face Transformers","text":"<pre><code>from transformers import AutoModelForCausalLM\nfrom rotalabs_accel import TritonRMSNorm\n\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n\n# Replace all RMSNorm layers\nfor layer in model.model.layers:\n    layer.input_layernorm = TritonRMSNorm(\n        layer.input_layernorm.weight.shape[0]\n    )\n    layer.post_attention_layernorm = TritonRMSNorm(\n        layer.post_attention_layernorm.weight.shape[0]\n    )\n</code></pre>"},{"location":"api/quantization/","title":"Quantization","text":"<p>INT8 symmetric quantization utilities for efficient inference.</p>"},{"location":"api/quantization/#overview","title":"Overview","text":"<p>This module provides utilities for quantizing neural network weights to INT8 format, enabling memory-efficient inference with minimal accuracy loss.</p>"},{"location":"api/quantization/#quantization-scheme","title":"Quantization Scheme","text":"<p>We use symmetric quantization with the following formula:</p> <pre><code>scale = max(|tensor|) / 127\nquantized = round(tensor / scale).clamp(-128, 127).to(int8)\n</code></pre> <p>To dequantize:</p> <pre><code>dequantized = quantized.float() * scale\n</code></pre>"},{"location":"api/quantization/#benefits","title":"Benefits","text":"Aspect FP16 INT8 Memory per weight 2 bytes 1 byte Memory reduction - 50% Accuracy Baseline ~99.5%+ of baseline"},{"location":"api/quantization/#use-cases","title":"Use Cases","text":"<ul> <li>Large model inference: Fit bigger models in GPU memory</li> <li>Deployment: Reduce model size for edge devices</li> <li>Batched inference: Handle more concurrent requests</li> </ul>"},{"location":"api/quantization/#quick-start","title":"Quick Start","text":"<pre><code>from rotalabs_accel import (\n    quantize_symmetric,\n    dequantize,\n    quantize_weight_per_channel,\n    calculate_quantization_error,\n    QuantizedLinear,\n)\n\n# Quantize a weight tensor\nweight = torch.randn(4096, 4096, dtype=torch.float16)\nweight_int8, scale = quantize_symmetric(weight)\n\n# Check accuracy\nerrors = calculate_quantization_error(weight, weight_int8, scale)\nprint(f\"SNR: {errors['snr_db']:.1f} dB\")\n\n# Use in a model\nlinear = torch.nn.Linear(4096, 4096)\nqlinear = QuantizedLinear.from_linear(linear)\n</code></pre>"},{"location":"api/quantization/#quantization-granularity","title":"Quantization Granularity","text":""},{"location":"api/quantization/#per-tensor-quantization","title":"Per-Tensor Quantization","text":"<p>One scale for the entire tensor. Fastest but lowest accuracy.</p> <pre><code>weight_int8, scale = quantize_symmetric(weight)\n# scale.shape: ()\n</code></pre>"},{"location":"api/quantization/#per-channel-quantization","title":"Per-Channel Quantization","text":"<p>One scale per output channel. Better accuracy, minimal overhead.</p> <pre><code>weight_int8, scales = quantize_weight_per_channel(weight)\n# scales.shape: (out_features,)\n</code></pre> <p>Recommendation: Use per-channel for best accuracy/speed tradeoff.</p>"},{"location":"api/quantization/#api-reference","title":"API Reference","text":""},{"location":"api/quantization/#functions","title":"Functions","text":""},{"location":"api/quantization/#rotalabs_accel.quantization.symmetric.quantize_symmetric","title":"quantize_symmetric","text":"<pre><code>quantize_symmetric(\n    tensor: Tensor, bits: int = 8, dim: Optional[int] = None\n) -&gt; tuple[torch.Tensor, torch.Tensor]\n</code></pre> <p>Symmetric quantization of tensor to INT8.</p> <p>Computes per-tensor or per-channel quantization using symmetric scheme: - scale = max(|tensor|) / (2^(bits-1) - 1) - quantized = round(tensor / scale).clamp(-128, 127)</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>Input tensor to quantize (typically FP16 or FP32 weights).</p> required <code>bits</code> <code>int</code> <p>Number of bits for quantization (default: 8).</p> <code>8</code> <code>dim</code> <code>Optional[int]</code> <p>Dimension for per-channel quantization. If None, uses per-tensor.  For weight matrices (out_features, in_features), use dim=0 for  per-output-channel quantization.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Tuple of (quantized_int8, scale):</p> <code>Tensor</code> <ul> <li>quantized_int8: INT8 tensor of same shape as input</li> </ul> <code>tuple[Tensor, Tensor]</code> <ul> <li>scale: FP32 scale factor(s) for dequantization</li> </ul> Example <p>weight = torch.randn(4096, 4096, dtype=torch.float16) weight_int8, scale = quantize_symmetric(weight) weight_fp16 = dequantize(weight_int8, scale) error = (weight - weight_fp16).abs().max()</p> Source code in <code>src/rotalabs_accel/quantization/symmetric.py</code> <pre><code>def quantize_symmetric(\n    tensor: torch.Tensor,\n    bits: int = 8,\n    dim: Optional[int] = None,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Symmetric quantization of tensor to INT8.\n\n    Computes per-tensor or per-channel quantization using symmetric scheme:\n    - scale = max(|tensor|) / (2^(bits-1) - 1)\n    - quantized = round(tensor / scale).clamp(-128, 127)\n\n    Args:\n        tensor: Input tensor to quantize (typically FP16 or FP32 weights).\n        bits: Number of bits for quantization (default: 8).\n        dim: Dimension for per-channel quantization. If None, uses per-tensor.\n             For weight matrices (out_features, in_features), use dim=0 for\n             per-output-channel quantization.\n\n    Returns:\n        Tuple of (quantized_int8, scale):\n        - quantized_int8: INT8 tensor of same shape as input\n        - scale: FP32 scale factor(s) for dequantization\n\n    Example:\n        &gt;&gt;&gt; weight = torch.randn(4096, 4096, dtype=torch.float16)\n        &gt;&gt;&gt; weight_int8, scale = quantize_symmetric(weight)\n        &gt;&gt;&gt; weight_fp16 = dequantize(weight_int8, scale)\n        &gt;&gt;&gt; error = (weight - weight_fp16).abs().max()\n    \"\"\"\n    assert bits == 8, f\"Only 8-bit quantization supported, got {bits}\"\n\n    # Max value for symmetric INT8: 127 (we use symmetric range -127 to 127)\n    qmax = 127\n\n    # Compute scale\n    if dim is None:\n        # Per-tensor quantization\n        max_val = tensor.abs().max()\n        scale = max_val / qmax\n    else:\n        # Per-channel quantization\n        max_val = tensor.abs().amax(dim=dim, keepdim=True)\n        scale = max_val / qmax\n\n    # Avoid division by zero\n    scale = torch.where(scale == 0, torch.ones_like(scale), scale)\n\n    # Quantize: round and clamp\n    quantized = torch.round(tensor / scale).clamp(-128, 127).to(torch.int8)\n\n    # Scale should be FP32 for numerical precision during dequantization\n    scale = scale.float()\n\n    # Squeeze scale if per-channel\n    if dim is not None:\n        scale = scale.squeeze(dim)\n\n    return quantized, scale\n</code></pre>"},{"location":"api/quantization/#rotalabs_accel.quantization.symmetric.dequantize","title":"dequantize","text":"<pre><code>dequantize(\n    quantized: Tensor,\n    scale: Tensor,\n    dtype: dtype = torch.float16,\n    dim: Optional[int] = None,\n) -&gt; torch.Tensor\n</code></pre> <p>Dequantize INT8 tensor back to floating point.</p> <p>Parameters:</p> Name Type Description Default <code>quantized</code> <code>Tensor</code> <p>INT8 quantized tensor.</p> required <code>scale</code> <code>Tensor</code> <p>Scale factor(s) from quantization.</p> required <code>dtype</code> <code>dtype</code> <p>Output dtype (default: float16).</p> <code>float16</code> <code>dim</code> <code>Optional[int]</code> <p>Dimension along which scale was computed (for broadcasting).  For weight matrices with per-output-channel quantization, use dim=0.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Dequantized tensor in specified dtype.</p> Example <p>weight_int8, scale = quantize_symmetric(weight_fp16) weight_restored = dequantize(weight_int8, scale)</p> Source code in <code>src/rotalabs_accel/quantization/symmetric.py</code> <pre><code>def dequantize(\n    quantized: torch.Tensor,\n    scale: torch.Tensor,\n    dtype: torch.dtype = torch.float16,\n    dim: Optional[int] = None,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Dequantize INT8 tensor back to floating point.\n\n    Args:\n        quantized: INT8 quantized tensor.\n        scale: Scale factor(s) from quantization.\n        dtype: Output dtype (default: float16).\n        dim: Dimension along which scale was computed (for broadcasting).\n             For weight matrices with per-output-channel quantization, use dim=0.\n\n    Returns:\n        Dequantized tensor in specified dtype.\n\n    Example:\n        &gt;&gt;&gt; weight_int8, scale = quantize_symmetric(weight_fp16)\n        &gt;&gt;&gt; weight_restored = dequantize(weight_int8, scale)\n    \"\"\"\n    # Convert to float for multiplication\n    quantized_float = quantized.float()\n\n    # Handle per-channel scale broadcasting\n    if scale.dim() == 1 and quantized.dim() == 2:\n        if dim is None or dim == 0:\n            # Scale has shape (out_features,), need (out_features, 1)\n            scale = scale.unsqueeze(1)\n        else:\n            # Scale has shape (in_features,), need (1, in_features)\n            scale = scale.unsqueeze(0)\n\n    # Dequantize\n    dequantized = quantized_float * scale\n\n    return dequantized.to(dtype)\n</code></pre>"},{"location":"api/quantization/#rotalabs_accel.quantization.symmetric.quantize_weight_per_channel","title":"quantize_weight_per_channel","text":"<pre><code>quantize_weight_per_channel(\n    weight: Tensor,\n) -&gt; tuple[torch.Tensor, torch.Tensor]\n</code></pre> <p>Quantize weight matrix with per-output-channel scales.</p> <p>For a weight matrix of shape (out_features, in_features), computes one scale per output channel (row).</p> <p>This is the common scheme for W8A16 inference, providing good accuracy while allowing efficient dequantization during matmul.</p> <p>Parameters:</p> Name Type Description Default <code>weight</code> <code>Tensor</code> <p>Weight tensor of shape (out_features, in_features).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tuple of (weight_int8, scales):</p> <code>Tensor</code> <ul> <li>weight_int8: INT8 weights of shape (out_features, in_features)</li> </ul> <code>tuple[Tensor, Tensor]</code> <ul> <li>scales: FP32 scales of shape (out_features,)</li> </ul> Example <p>W = torch.randn(4096, 4096, dtype=torch.float16) W_int8, scales = quantize_weight_per_channel(W)</p> Source code in <code>src/rotalabs_accel/quantization/symmetric.py</code> <pre><code>def quantize_weight_per_channel(\n    weight: torch.Tensor,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Quantize weight matrix with per-output-channel scales.\n\n    For a weight matrix of shape (out_features, in_features), computes\n    one scale per output channel (row).\n\n    This is the common scheme for W8A16 inference, providing good accuracy\n    while allowing efficient dequantization during matmul.\n\n    Args:\n        weight: Weight tensor of shape (out_features, in_features).\n\n    Returns:\n        Tuple of (weight_int8, scales):\n        - weight_int8: INT8 weights of shape (out_features, in_features)\n        - scales: FP32 scales of shape (out_features,)\n\n    Example:\n        &gt;&gt;&gt; W = torch.randn(4096, 4096, dtype=torch.float16)\n        &gt;&gt;&gt; W_int8, scales = quantize_weight_per_channel(W)\n    \"\"\"\n    assert weight.dim() == 2, f\"Expected 2D weight matrix, got {weight.dim()}D\"\n    return quantize_symmetric(weight, bits=8, dim=1)\n</code></pre>"},{"location":"api/quantization/#rotalabs_accel.quantization.symmetric.calculate_quantization_error","title":"calculate_quantization_error","text":"<pre><code>calculate_quantization_error(\n    original: Tensor,\n    quantized: Tensor,\n    scale: Tensor,\n    dim: Optional[int] = None,\n) -&gt; dict[str, float]\n</code></pre> <p>Calculate quantization error metrics.</p> <p>Parameters:</p> Name Type Description Default <code>original</code> <code>Tensor</code> <p>Original FP tensor.</p> required <code>quantized</code> <code>Tensor</code> <p>INT8 quantized tensor.</p> required <code>scale</code> <code>Tensor</code> <p>Scale factor(s).</p> required <code>dim</code> <code>Optional[int]</code> <p>Dimension for scale broadcasting.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>Dictionary with error metrics:</p> <code>dict[str, float]</code> <ul> <li>max_abs_error: Maximum absolute error</li> </ul> <code>dict[str, float]</code> <ul> <li>mean_abs_error: Mean absolute error</li> </ul> <code>dict[str, float]</code> <ul> <li>relative_error: Max relative error (percentage)</li> </ul> <code>dict[str, float]</code> <ul> <li>snr_db: Signal-to-noise ratio in dB</li> </ul> Source code in <code>src/rotalabs_accel/quantization/symmetric.py</code> <pre><code>def calculate_quantization_error(\n    original: torch.Tensor,\n    quantized: torch.Tensor,\n    scale: torch.Tensor,\n    dim: Optional[int] = None,\n) -&gt; dict[str, float]:\n    \"\"\"\n    Calculate quantization error metrics.\n\n    Args:\n        original: Original FP tensor.\n        quantized: INT8 quantized tensor.\n        scale: Scale factor(s).\n        dim: Dimension for scale broadcasting.\n\n    Returns:\n        Dictionary with error metrics:\n        - max_abs_error: Maximum absolute error\n        - mean_abs_error: Mean absolute error\n        - relative_error: Max relative error (percentage)\n        - snr_db: Signal-to-noise ratio in dB\n    \"\"\"\n    # Dequantize\n    restored = dequantize(quantized, scale, dtype=original.dtype, dim=dim)\n\n    # Calculate errors\n    diff = (original - restored).float()\n    original_float = original.float()\n\n    max_abs_error = diff.abs().max().item()\n    mean_abs_error = diff.abs().mean().item()\n\n    # Relative error: only consider significant values (&gt; 10% of max) to avoid\n    # misleading high errors for small values. For 8-bit quantization, the max\n    # relative error is ~scale/(2*value), so small values inherently have high\n    # relative error even with perfect quantization.\n    threshold = original_float.abs().max() * 0.1\n    significant_mask = original_float.abs() &gt; threshold\n    if significant_mask.any():\n        rel_error = (diff[significant_mask].abs() / original_float[significant_mask].abs()).max().item() * 100\n    else:\n        rel_error = 0.0\n\n    # Signal-to-noise ratio\n    signal_power = (original_float ** 2).mean()\n    noise_power = (diff ** 2).mean()\n    snr_db = 10 * torch.log10(signal_power / (noise_power + 1e-10)).item()\n\n    return {\n        \"max_abs_error\": max_abs_error,\n        \"mean_abs_error\": mean_abs_error,\n        \"relative_error_pct\": rel_error,\n        \"snr_db\": snr_db,\n    }\n</code></pre>"},{"location":"api/quantization/#classes","title":"Classes","text":""},{"location":"api/quantization/#rotalabs_accel.quantization.symmetric.QuantizedLinear","title":"QuantizedLinear","text":"<p>               Bases: <code>Module</code></p> <p>Linear layer with INT8 quantized weights.</p> <p>Stores weights in INT8 format and dequantizes on-the-fly during forward pass. This is a reference implementation - the actual kernel-level optimization happens in the Triton INT8 GEMM kernel.</p> <p>For W8A16 inference: - Weights: INT8 (2x memory reduction) - Activations: FP16 - Compute: FP16 with FP32 accumulation</p> <p>Parameters:</p> Name Type Description Default <code>in_features</code> <code>int</code> <p>Input dimension.</p> required <code>out_features</code> <code>int</code> <p>Output dimension.</p> required <code>bias</code> <code>bool</code> <p>Whether to include bias (default: False for LLM weights).</p> <code>False</code> Example <p>linear = QuantizedLinear(4096, 4096) linear.quantize_weights(pretrained_weight) y = linear(x)  # x is FP16, y is FP16</p> Source code in <code>src/rotalabs_accel/quantization/symmetric.py</code> <pre><code>class QuantizedLinear(torch.nn.Module):\n    \"\"\"\n    Linear layer with INT8 quantized weights.\n\n    Stores weights in INT8 format and dequantizes on-the-fly during forward pass.\n    This is a reference implementation - the actual kernel-level optimization\n    happens in the Triton INT8 GEMM kernel.\n\n    For W8A16 inference:\n    - Weights: INT8 (2x memory reduction)\n    - Activations: FP16\n    - Compute: FP16 with FP32 accumulation\n\n    Args:\n        in_features: Input dimension.\n        out_features: Output dimension.\n        bias: Whether to include bias (default: False for LLM weights).\n\n    Example:\n        &gt;&gt;&gt; linear = QuantizedLinear(4096, 4096)\n        &gt;&gt;&gt; linear.quantize_weights(pretrained_weight)\n        &gt;&gt;&gt; y = linear(x)  # x is FP16, y is FP16\n    \"\"\"\n\n    def __init__(\n        self,\n        in_features: int,\n        out_features: int,\n        bias: bool = False,\n    ):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n\n        # Register buffers for quantized weights\n        self.register_buffer(\n            \"weight_int8\",\n            torch.zeros(out_features, in_features, dtype=torch.int8)\n        )\n        self.register_buffer(\n            \"weight_scale\",\n            torch.ones(out_features, dtype=torch.float32)\n        )\n\n        if bias:\n            self.bias = torch.nn.Parameter(torch.zeros(out_features))\n        else:\n            self.register_parameter(\"bias\", None)\n\n        self._quantized = False\n\n    def quantize_weights(self, weight: torch.Tensor) -&gt; None:\n        \"\"\"\n        Quantize and store weights.\n\n        Args:\n            weight: FP16 or FP32 weight tensor of shape (out_features, in_features).\n        \"\"\"\n        assert weight.shape == (self.out_features, self.in_features)\n        weight_int8, scale = quantize_weight_per_channel(weight)\n        self.weight_int8.copy_(weight_int8)\n        self.weight_scale.copy_(scale)\n        self._quantized = True\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass with on-the-fly dequantization.\n\n        Args:\n            x: Input tensor of shape (..., in_features).\n\n        Returns:\n            Output tensor of shape (..., out_features).\n        \"\"\"\n        if not self._quantized:\n            raise RuntimeError(\"Weights not quantized. Call quantize_weights() first.\")\n\n        # Dequantize weights\n        weight = dequantize(self.weight_int8, self.weight_scale, dtype=x.dtype, dim=0)\n\n        # Compute matmul\n        y = torch.nn.functional.linear(x, weight, self.bias)\n\n        return y\n\n    @classmethod\n    def from_linear(cls, linear: torch.nn.Linear) -&gt; \"QuantizedLinear\":\n        \"\"\"\n        Create QuantizedLinear from existing nn.Linear.\n\n        Args:\n            linear: Pretrained linear layer.\n\n        Returns:\n            QuantizedLinear with quantized weights.\n        \"\"\"\n        has_bias = linear.bias is not None\n        quantized = cls(linear.in_features, linear.out_features, bias=has_bias)\n        quantized.quantize_weights(linear.weight.data)\n        if has_bias:\n            quantized.bias.data.copy_(linear.bias.data)\n        return quantized\n\n    def extra_repr(self) -&gt; str:\n        return (\n            f\"in_features={self.in_features}, out_features={self.out_features}, \"\n            f\"bias={self.bias is not None}, quantized={self._quantized}\"\n        )\n</code></pre>"},{"location":"api/quantization/#rotalabs_accel.quantization.symmetric.QuantizedLinear.__init__","title":"__init__","text":"<pre><code>__init__(\n    in_features: int, out_features: int, bias: bool = False\n)\n</code></pre> Source code in <code>src/rotalabs_accel/quantization/symmetric.py</code> <pre><code>def __init__(\n    self,\n    in_features: int,\n    out_features: int,\n    bias: bool = False,\n):\n    super().__init__()\n    self.in_features = in_features\n    self.out_features = out_features\n\n    # Register buffers for quantized weights\n    self.register_buffer(\n        \"weight_int8\",\n        torch.zeros(out_features, in_features, dtype=torch.int8)\n    )\n    self.register_buffer(\n        \"weight_scale\",\n        torch.ones(out_features, dtype=torch.float32)\n    )\n\n    if bias:\n        self.bias = torch.nn.Parameter(torch.zeros(out_features))\n    else:\n        self.register_parameter(\"bias\", None)\n\n    self._quantized = False\n</code></pre>"},{"location":"api/quantization/#rotalabs_accel.quantization.symmetric.QuantizedLinear.quantize_weights","title":"quantize_weights","text":"<pre><code>quantize_weights(weight: Tensor) -&gt; None\n</code></pre> <p>Quantize and store weights.</p> <p>Parameters:</p> Name Type Description Default <code>weight</code> <code>Tensor</code> <p>FP16 or FP32 weight tensor of shape (out_features, in_features).</p> required Source code in <code>src/rotalabs_accel/quantization/symmetric.py</code> <pre><code>def quantize_weights(self, weight: torch.Tensor) -&gt; None:\n    \"\"\"\n    Quantize and store weights.\n\n    Args:\n        weight: FP16 or FP32 weight tensor of shape (out_features, in_features).\n    \"\"\"\n    assert weight.shape == (self.out_features, self.in_features)\n    weight_int8, scale = quantize_weight_per_channel(weight)\n    self.weight_int8.copy_(weight_int8)\n    self.weight_scale.copy_(scale)\n    self._quantized = True\n</code></pre>"},{"location":"api/quantization/#rotalabs_accel.quantization.symmetric.QuantizedLinear.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Forward pass with on-the-fly dequantization.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (..., in_features).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor of shape (..., out_features).</p> Source code in <code>src/rotalabs_accel/quantization/symmetric.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass with on-the-fly dequantization.\n\n    Args:\n        x: Input tensor of shape (..., in_features).\n\n    Returns:\n        Output tensor of shape (..., out_features).\n    \"\"\"\n    if not self._quantized:\n        raise RuntimeError(\"Weights not quantized. Call quantize_weights() first.\")\n\n    # Dequantize weights\n    weight = dequantize(self.weight_int8, self.weight_scale, dtype=x.dtype, dim=0)\n\n    # Compute matmul\n    y = torch.nn.functional.linear(x, weight, self.bias)\n\n    return y\n</code></pre>"},{"location":"api/quantization/#rotalabs_accel.quantization.symmetric.QuantizedLinear.from_linear","title":"from_linear  <code>classmethod</code>","text":"<pre><code>from_linear(linear: Linear) -&gt; QuantizedLinear\n</code></pre> <p>Create QuantizedLinear from existing nn.Linear.</p> <p>Parameters:</p> Name Type Description Default <code>linear</code> <code>Linear</code> <p>Pretrained linear layer.</p> required <p>Returns:</p> Type Description <code>QuantizedLinear</code> <p>QuantizedLinear with quantized weights.</p> Source code in <code>src/rotalabs_accel/quantization/symmetric.py</code> <pre><code>@classmethod\ndef from_linear(cls, linear: torch.nn.Linear) -&gt; \"QuantizedLinear\":\n    \"\"\"\n    Create QuantizedLinear from existing nn.Linear.\n\n    Args:\n        linear: Pretrained linear layer.\n\n    Returns:\n        QuantizedLinear with quantized weights.\n    \"\"\"\n    has_bias = linear.bias is not None\n    quantized = cls(linear.in_features, linear.out_features, bias=has_bias)\n    quantized.quantize_weights(linear.weight.data)\n    if has_bias:\n        quantized.bias.data.copy_(linear.bias.data)\n    return quantized\n</code></pre>"},{"location":"api/quantization/#best-practices","title":"Best Practices","text":""},{"location":"api/quantization/#1-quantize-after-training","title":"1. Quantize After Training","text":"<p>Quantize pretrained weights, not randomly initialized ones:</p> <pre><code># Good: quantize pretrained weights\nmodel = load_pretrained_model()\nfor module in model.modules():\n    if isinstance(module, nn.Linear):\n        qmodule = QuantizedLinear.from_linear(module)\n        # replace module with qmodule\n</code></pre>"},{"location":"api/quantization/#2-evaluate-before-deployment","title":"2. Evaluate Before Deployment","text":"<p>Always check quantization accuracy on your specific model:</p> <pre><code># Run validation before and after quantization\nbaseline_loss = evaluate(model)\nquantize_model(model)\nquantized_loss = evaluate(model)\nprint(f\"Loss increase: {quantized_loss - baseline_loss:.4f}\")\n</code></pre>"},{"location":"api/quantization/#3-keep-certain-layers-in-fp16","title":"3. Keep Certain Layers in FP16","text":"<p>Some layers are more sensitive to quantization:</p> <ul> <li>First and last layers</li> <li>Layers with small weight magnitudes</li> <li>Attention output projections</li> </ul> <pre><code># Skip quantizing sensitive layers\nfor name, module in model.named_modules():\n    if \"lm_head\" in name or \"embed\" in name:\n        continue  # Keep in FP16\n    if isinstance(module, nn.Linear):\n        # Quantize\n</code></pre>"},{"location":"api/quantization/#error-metrics","title":"Error Metrics","text":"<p>The <code>calculate_quantization_error</code> function returns:</p> Metric Description Typical Value <code>max_abs_error</code> Maximum absolute difference &lt; 0.02 <code>mean_abs_error</code> Mean absolute difference &lt; 0.005 <code>relative_error_pct</code> Max relative error for significant values &lt; 1% <code>snr_db</code> Signal-to-noise ratio &gt; 40 dB <p>Values may vary based on weight distribution. Lower SNR indicates more quantization error.</p>"},{"location":"api/utils/","title":"Utilities","text":"<p>Device detection and capability checking utilities.</p>"},{"location":"api/utils/#overview","title":"Overview","text":"<p>These utilities help you write portable code that adapts to available hardware.</p> <pre><code>from rotalabs_accel import (\n    get_device,\n    is_cuda_available,\n    is_triton_available,\n    get_device_properties,\n)\n\n# Auto-select best device\ndevice = get_device()  # Returns 'cuda' if available, else 'cpu'\n\n# Check capabilities\nprint(f\"CUDA available: {is_cuda_available()}\")\nprint(f\"Triton available: {is_triton_available()}\")\n\n# Get detailed GPU info\nif is_cuda_available():\n    props = get_device_properties()\n    print(f\"GPU: {props['name']}\")\n    print(f\"VRAM: {props['total_memory'] / 1e9:.1f} GB\")\n</code></pre>"},{"location":"api/utils/#api-reference","title":"API Reference","text":""},{"location":"api/utils/#functions","title":"Functions","text":""},{"location":"api/utils/#rotalabs_accel.utils.device.get_device","title":"get_device","text":"<pre><code>get_device(device: Optional[str] = None) -&gt; torch.device\n</code></pre> <p>Get a torch device, with smart defaults.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>Optional[str]</code> <p>Device string ('cuda', 'cpu', 'cuda:0', etc.).     If None, returns CUDA if available, else CPU.</p> <code>None</code> <p>Returns:</p> Type Description <code>device</code> <p>torch.device instance.</p> Example <p>device = get_device()  # Auto-detect device = get_device('cuda:1')  # Specific GPU</p> Source code in <code>src/rotalabs_accel/utils/device.py</code> <pre><code>def get_device(device: Optional[str] = None) -&gt; torch.device:\n    \"\"\"\n    Get a torch device, with smart defaults.\n\n    Args:\n        device: Device string ('cuda', 'cpu', 'cuda:0', etc.).\n                If None, returns CUDA if available, else CPU.\n\n    Returns:\n        torch.device instance.\n\n    Example:\n        &gt;&gt;&gt; device = get_device()  # Auto-detect\n        &gt;&gt;&gt; device = get_device('cuda:1')  # Specific GPU\n    \"\"\"\n    if device is not None:\n        return torch.device(device)\n\n    if is_cuda_available():\n        return torch.device('cuda')\n    return torch.device('cpu')\n</code></pre>"},{"location":"api/utils/#rotalabs_accel.utils.device.is_cuda_available","title":"is_cuda_available","text":"<pre><code>is_cuda_available() -&gt; bool\n</code></pre> <p>Check if CUDA is available.</p> Source code in <code>src/rotalabs_accel/utils/device.py</code> <pre><code>def is_cuda_available() -&gt; bool:\n    \"\"\"Check if CUDA is available.\"\"\"\n    return torch.cuda.is_available()\n</code></pre>"},{"location":"api/utils/#rotalabs_accel.utils.device.is_triton_available","title":"is_triton_available","text":"<pre><code>is_triton_available() -&gt; bool\n</code></pre> <p>Check if Triton is available.</p> Source code in <code>src/rotalabs_accel/utils/device.py</code> <pre><code>def is_triton_available() -&gt; bool:\n    \"\"\"Check if Triton is available.\"\"\"\n    try:\n        import triton\n        return True\n    except ImportError:\n        return False\n</code></pre>"},{"location":"api/utils/#rotalabs_accel.utils.device.get_device_properties","title":"get_device_properties","text":"<pre><code>get_device_properties(\n    device: Optional[device] = None,\n) -&gt; Dict[str, Any]\n</code></pre> <p>Get device properties and capabilities.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>Optional[device]</code> <p>Device to query. If None, uses current CUDA device.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with device properties:</p> <code>Dict[str, Any]</code> <ul> <li>name: Device name</li> </ul> <code>Dict[str, Any]</code> <ul> <li>compute_capability: (major, minor) tuple</li> </ul> <code>Dict[str, Any]</code> <ul> <li>total_memory: Total memory in bytes</li> </ul> <code>Dict[str, Any]</code> <ul> <li>supports_fp16: Whether FP16 is supported</li> </ul> <code>Dict[str, Any]</code> <ul> <li>supports_bf16: Whether BF16 is supported</li> </ul> <code>Dict[str, Any]</code> <ul> <li>supports_fp8: Whether FP8 is supported (Hopper+)</li> </ul> <code>Dict[str, Any]</code> <ul> <li>supports_int8_tensor_cores: Whether INT8 tensor cores available</li> </ul> Example <p>props = get_device_properties() print(f\"GPU: {props['name']}\") if props['supports_fp8']: ...     print(\"FP8 quantization available!\")</p> Source code in <code>src/rotalabs_accel/utils/device.py</code> <pre><code>def get_device_properties(device: Optional[torch.device] = None) -&gt; Dict[str, Any]:\n    \"\"\"\n    Get device properties and capabilities.\n\n    Args:\n        device: Device to query. If None, uses current CUDA device.\n\n    Returns:\n        Dictionary with device properties:\n        - name: Device name\n        - compute_capability: (major, minor) tuple\n        - total_memory: Total memory in bytes\n        - supports_fp16: Whether FP16 is supported\n        - supports_bf16: Whether BF16 is supported\n        - supports_fp8: Whether FP8 is supported (Hopper+)\n        - supports_int8_tensor_cores: Whether INT8 tensor cores available\n\n    Example:\n        &gt;&gt;&gt; props = get_device_properties()\n        &gt;&gt;&gt; print(f\"GPU: {props['name']}\")\n        &gt;&gt;&gt; if props['supports_fp8']:\n        ...     print(\"FP8 quantization available!\")\n    \"\"\"\n    if not is_cuda_available():\n        return {\n            'name': 'CPU',\n            'compute_capability': (0, 0),\n            'total_memory': 0,\n            'supports_fp16': True,\n            'supports_bf16': False,\n            'supports_fp8': False,\n            'supports_int8_tensor_cores': False,\n        }\n\n    if device is None:\n        device = torch.device('cuda')\n\n    props = torch.cuda.get_device_properties(device)\n    cc = (props.major, props.minor)\n\n    return {\n        'name': props.name,\n        'compute_capability': cc,\n        'total_memory': props.total_memory,\n        'supports_fp16': cc &gt;= (5, 3),  # Maxwell+\n        'supports_bf16': cc &gt;= (8, 0),  # Ampere+\n        'supports_fp8': cc &gt;= (8, 9),   # Hopper (sm89) / Ada (sm89)\n        'supports_int8_tensor_cores': cc &gt;= (7, 5),  # Turing+\n        'multi_processor_count': props.multi_processor_count,\n    }\n</code></pre>"},{"location":"api/utils/#rotalabs_accel.utils.device.select_dtype","title":"select_dtype","text":"<pre><code>select_dtype(\n    preferred: dtype = torch.float16,\n    device: Optional[device] = None,\n) -&gt; torch.dtype\n</code></pre> <p>Select the best available dtype for the device.</p> <p>Parameters:</p> Name Type Description Default <code>preferred</code> <code>dtype</code> <p>Preferred dtype if supported.</p> <code>float16</code> <code>device</code> <code>Optional[device]</code> <p>Device to check capabilities for.</p> <code>None</code> <p>Returns:</p> Type Description <code>dtype</code> <p>Best supported dtype.</p> Example <p>dtype = select_dtype(torch.bfloat16) model = model.to(dtype)</p> Source code in <code>src/rotalabs_accel/utils/device.py</code> <pre><code>def select_dtype(\n    preferred: torch.dtype = torch.float16,\n    device: Optional[torch.device] = None,\n) -&gt; torch.dtype:\n    \"\"\"\n    Select the best available dtype for the device.\n\n    Args:\n        preferred: Preferred dtype if supported.\n        device: Device to check capabilities for.\n\n    Returns:\n        Best supported dtype.\n\n    Example:\n        &gt;&gt;&gt; dtype = select_dtype(torch.bfloat16)\n        &gt;&gt;&gt; model = model.to(dtype)\n    \"\"\"\n    props = get_device_properties(device)\n\n    if preferred == torch.bfloat16 and not props['supports_bf16']:\n        return torch.float16\n\n    if preferred == torch.float16 and not props['supports_fp16']:\n        return torch.float32\n\n    return preferred\n</code></pre>"},{"location":"api/utils/#usage-patterns","title":"Usage Patterns","text":""},{"location":"api/utils/#portable-device-selection","title":"Portable Device Selection","text":"<pre><code>from rotalabs_accel import get_device\n\ndevice = get_device()\n\n# Works on any platform\nmodel = Model().to(device)\nx = torch.randn(1, 512, 4096, device=device)\ny = model(x)\n</code></pre>"},{"location":"api/utils/#conditional-logic-based-on-capabilities","title":"Conditional Logic Based on Capabilities","text":"<pre><code>from rotalabs_accel import is_triton_available, get_device_properties\n\nif is_triton_available():\n    print(\"Using Triton-optimized kernels\")\nelse:\n    print(\"Falling back to PyTorch\")\n\n# Select dtype based on GPU capabilities\nif is_cuda_available():\n    props = get_device_properties()\n    if props.get('supports_bf16', False):\n        dtype = torch.bfloat16\n        print(\"Using BF16 (Ampere+)\")\n    else:\n        dtype = torch.float16\n        print(\"Using FP16\")\nelse:\n    dtype = torch.float32\n    print(\"Using FP32 on CPU\")\n</code></pre>"},{"location":"api/utils/#multi-gpu-selection","title":"Multi-GPU Selection","text":"<pre><code>from rotalabs_accel import get_device\n\n# Select specific GPU\ndevice = get_device(\"cuda:0\")\ndevice = get_device(\"cuda:1\")\n\n# Force CPU even if GPU available\ndevice = get_device(\"cpu\")\n</code></pre>"},{"location":"api/utils/#device-properties","title":"Device Properties","text":"<p>The <code>get_device_properties()</code> function returns a dictionary with:</p> Property Type Description <code>name</code> str GPU name (e.g., \"NVIDIA A100-SXM4-80GB\") <code>compute_capability</code> tuple Compute capability (e.g., (8, 0)) <code>total_memory</code> int Total VRAM in bytes <code>supports_bf16</code> bool BF16 tensor core support (Ampere+) <code>supports_fp8</code> bool FP8 support (Hopper+)"},{"location":"api/utils/#gpu-generation-detection","title":"GPU Generation Detection","text":"<pre><code>props = get_device_properties()\ncc = props['compute_capability']\n\nif cc &gt;= (9, 0):\n    print(\"Hopper (H100) - FP8 support\")\nelif cc &gt;= (8, 0):\n    print(\"Ampere (A100/A10) - BF16 tensor cores\")\nelif cc &gt;= (7, 0):\n    print(\"Volta/Turing (V100/T4)\")\nelse:\n    print(\"Older GPU\")\n</code></pre>"},{"location":"api/utils/#triton-availability","title":"Triton Availability","text":"<p>Triton requires:</p> <ul> <li>Linux operating system</li> <li>NVIDIA GPU with CUDA</li> <li>Python 3.8+</li> </ul> <p>On other platforms, <code>is_triton_available()</code> returns <code>False</code> and all kernels automatically fall back to PyTorch.</p> <pre><code>from rotalabs_accel import is_triton_available\n\nif not is_triton_available():\n    # Could be:\n    # - macOS/Windows (Triton only supports Linux)\n    # - No NVIDIA GPU\n    # - Triton not installed: pip install triton\n    print(\"Triton not available, using PyTorch fallbacks\")\n</code></pre>"},{"location":"api/kernels/activations/","title":"Activations","text":"<p>SwiGLU (Swish-Gated Linear Unit) activation kernel used in modern LLMs.</p>"},{"location":"api/kernels/activations/#overview","title":"Overview","text":"<p>SwiGLU is a variant of GLU (Gated Linear Unit) that uses SiLU (Swish) as the activation function. It's used in LLaMA, PaLM, Mistral, and other state-of-the-art models.</p> <p>Mathematical formula:</p> \\[ \\text{SwiGLU}(x) = \\text{SiLU}(x W_{gate}) \\odot (x W_{up}) \\] <p>Where:</p> <ul> <li>\\(W_{gate}\\) and \\(W_{up}\\) are learned weight matrices</li> <li>\\(\\text{SiLU}(z) = z \\cdot \\sigma(z) = z \\cdot \\frac{1}{1 + e^{-z}}\\)</li> <li>\\(\\odot\\) is element-wise multiplication</li> </ul> <p>The full FFN layer in SwiGLU-based transformers is:</p> \\[ \\text{FFN}(x) = \\text{SwiGLU}(x) W_{down} \\]"},{"location":"api/kernels/activations/#performance-characteristics","title":"Performance Characteristics","text":"<p>SwiGLU activation is memory-bound with ~1.3 FLOPs/byte. The Triton kernel fuses the SiLU activation and element-wise multiply:</p> Configuration PyTorch Triton Speedup intermediate=11008, seq=2048 89 \u03bcs 31 \u03bcs 2.9x intermediate=16384, seq=2048 134 \u03bcs 48 \u03bcs 2.8x intermediate=11008, seq=8192 354 \u03bcs 123 \u03bcs 2.9x"},{"location":"api/kernels/activations/#usage-examples","title":"Usage Examples","text":""},{"location":"api/kernels/activations/#full-swiglu-ffn-module","title":"Full SwiGLU FFN Module","text":"<pre><code>import torch\nfrom rotalabs_accel import SwiGLU\n\n# Create SwiGLU FFN (includes gate, up, and down projections)\nffn = SwiGLU(\n    hidden_size=4096,       # Input/output dimension\n    intermediate_size=11008,  # Intermediate dimension (~2.7x hidden)\n    bias=False,             # Most LLMs don't use bias\n)\nffn = ffn.to(\"cuda\")\n\n# Forward pass\nx = torch.randn(2, 512, 4096, device=\"cuda\", dtype=torch.float16)\ny = ffn(x)  # Shape: (2, 512, 4096)\n</code></pre>"},{"location":"api/kernels/activations/#functional-api-after-your-own-projections","title":"Functional API (After Your Own Projections)","text":"<p>If you have your own projection layers:</p> <pre><code>from rotalabs_accel import swiglu_fused\n\n# Your custom projections\ngate = x @ W_gate.T  # Shape: (batch, seq, intermediate)\nup = x @ W_up.T      # Shape: (batch, seq, intermediate)\n\n# Fused activation\nactivated = swiglu_fused(gate, up)  # Shape: (batch, seq, intermediate)\n\n# Down projection\noutput = activated @ W_down.T  # Shape: (batch, seq, hidden)\n</code></pre>"},{"location":"api/kernels/activations/#integration-with-hugging-face-models","title":"Integration with Hugging Face Models","text":"<pre><code>from transformers import AutoModelForCausalLM\nfrom rotalabs_accel import SwiGLU\n\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n\n# Replace MLP layers with optimized SwiGLU\nfor layer in model.model.layers:\n    hidden_size = layer.mlp.gate_proj.in_features\n    intermediate_size = layer.mlp.gate_proj.out_features\n\n    optimized_mlp = SwiGLU(hidden_size, intermediate_size)\n    # Copy weights...\n\n    layer.mlp = optimized_mlp\n</code></pre>"},{"location":"api/kernels/activations/#api-reference","title":"API Reference","text":""},{"location":"api/kernels/activations/#functions","title":"Functions","text":""},{"location":"api/kernels/activations/#rotalabs_accel.kernels.activations.swiglu_fused","title":"swiglu_fused","text":"<pre><code>swiglu_fused(gate: Tensor, up: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Fused SwiGLU activation.</p> <p>Computes: y = silu(gate) * up</p> <p>Uses Triton kernel on CUDA when available, otherwise falls back to PyTorch.</p> <p>Parameters:</p> Name Type Description Default <code>gate</code> <code>Tensor</code> <p>Gate tensor of shape (...,), result of x @ W_gate projection.</p> required <code>up</code> <code>Tensor</code> <p>Up tensor of same shape as gate, result of x @ W_up projection.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor of same shape as inputs.</p> Example <p>gate = torch.randn(2, 8, 64) up = torch.randn(2, 8, 64) y = swiglu_fused(gate, up)</p> Source code in <code>src/rotalabs_accel/kernels/activations.py</code> <pre><code>def swiglu_fused(\n    gate: torch.Tensor,\n    up: torch.Tensor,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Fused SwiGLU activation.\n\n    Computes: y = silu(gate) * up\n\n    Uses Triton kernel on CUDA when available, otherwise falls back to PyTorch.\n\n    Args:\n        gate: Gate tensor of shape (...,), result of x @ W_gate projection.\n        up: Up tensor of same shape as gate, result of x @ W_up projection.\n\n    Returns:\n        Output tensor of same shape as inputs.\n\n    Example:\n        &gt;&gt;&gt; gate = torch.randn(2, 8, 64)\n        &gt;&gt;&gt; up = torch.randn(2, 8, 64)\n        &gt;&gt;&gt; y = swiglu_fused(gate, up)\n    \"\"\"\n    assert gate.shape == up.shape, f\"Shape mismatch: gate={gate.shape}, up={up.shape}\"\n\n    # Use Triton kernel if available and on CUDA\n    if HAS_TRITON and gate.is_cuda and up.is_cuda:\n        return _swiglu_triton(gate, up)\n\n    # Fallback to PyTorch\n    return swiglu_torch(gate, up)\n</code></pre>"},{"location":"api/kernels/activations/#rotalabs_accel.kernels.activations.swiglu_torch","title":"swiglu_torch","text":"<pre><code>swiglu_torch(gate: Tensor, up: Tensor) -&gt; torch.Tensor\n</code></pre> <p>PyTorch reference implementation of SwiGLU activation.</p> <p>Works on any device (CPU or CUDA).</p> Source code in <code>src/rotalabs_accel/kernels/activations.py</code> <pre><code>def swiglu_torch(\n    gate: torch.Tensor,\n    up: torch.Tensor,\n) -&gt; torch.Tensor:\n    \"\"\"\n    PyTorch reference implementation of SwiGLU activation.\n\n    Works on any device (CPU or CUDA).\n    \"\"\"\n    return torch.nn.functional.silu(gate) * up\n</code></pre>"},{"location":"api/kernels/activations/#classes","title":"Classes","text":""},{"location":"api/kernels/activations/#rotalabs_accel.kernels.activations.SwiGLU","title":"SwiGLU","text":"<p>               Bases: <code>Module</code></p> <p>SwiGLU module with linear projections.</p> Implements the full SwiGLU FFN <p>y = (silu(x @ W_gate) * (x @ W_up)) @ W_down</p> <p>Uses Triton kernel on CUDA when available, otherwise falls back to PyTorch.</p> <p>Parameters:</p> Name Type Description Default <code>hidden_size</code> <code>int</code> <p>Input/output dimension.</p> required <code>intermediate_size</code> <code>int</code> <p>Intermediate dimension for the FFN.</p> required <code>bias</code> <code>bool</code> <p>Whether to use bias in linear layers.</p> <code>False</code> Example <p>swiglu = SwiGLU(hidden_size=64, intermediate_size=256) x = torch.randn(2, 8, 64) y = swiglu(x)  # Shape: (2, 8, 64)</p> Source code in <code>src/rotalabs_accel/kernels/activations.py</code> <pre><code>class SwiGLU(torch.nn.Module):\n    \"\"\"\n    SwiGLU module with linear projections.\n\n    Implements the full SwiGLU FFN:\n        y = (silu(x @ W_gate) * (x @ W_up)) @ W_down\n\n    Uses Triton kernel on CUDA when available, otherwise falls back to PyTorch.\n\n    Args:\n        hidden_size: Input/output dimension.\n        intermediate_size: Intermediate dimension for the FFN.\n        bias: Whether to use bias in linear layers.\n\n    Example:\n        &gt;&gt;&gt; swiglu = SwiGLU(hidden_size=64, intermediate_size=256)\n        &gt;&gt;&gt; x = torch.randn(2, 8, 64)\n        &gt;&gt;&gt; y = swiglu(x)  # Shape: (2, 8, 64)\n    \"\"\"\n\n    def __init__(\n        self,\n        hidden_size: int,\n        intermediate_size: int,\n        bias: bool = False,\n    ):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.intermediate_size = intermediate_size\n\n        self.w_gate = torch.nn.Linear(hidden_size, intermediate_size, bias=bias)\n        self.w_up = torch.nn.Linear(hidden_size, intermediate_size, bias=bias)\n        self.w_down = torch.nn.Linear(intermediate_size, hidden_size, bias=bias)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        gate = self.w_gate(x)\n        up = self.w_up(x)\n        return self.w_down(swiglu_fused(gate, up))\n\n    def extra_repr(self) -&gt; str:\n        return f\"hidden_size={self.hidden_size}, intermediate_size={self.intermediate_size}\"\n</code></pre>"},{"location":"api/kernels/activations/#rotalabs_accel.kernels.activations.SwiGLU.__init__","title":"__init__","text":"<pre><code>__init__(\n    hidden_size: int,\n    intermediate_size: int,\n    bias: bool = False,\n)\n</code></pre> Source code in <code>src/rotalabs_accel/kernels/activations.py</code> <pre><code>def __init__(\n    self,\n    hidden_size: int,\n    intermediate_size: int,\n    bias: bool = False,\n):\n    super().__init__()\n    self.hidden_size = hidden_size\n    self.intermediate_size = intermediate_size\n\n    self.w_gate = torch.nn.Linear(hidden_size, intermediate_size, bias=bias)\n    self.w_up = torch.nn.Linear(hidden_size, intermediate_size, bias=bias)\n    self.w_down = torch.nn.Linear(intermediate_size, hidden_size, bias=bias)\n</code></pre>"},{"location":"api/kernels/activations/#rotalabs_accel.kernels.activations.SwiGLU.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; torch.Tensor\n</code></pre> Source code in <code>src/rotalabs_accel/kernels/activations.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    gate = self.w_gate(x)\n    up = self.w_up(x)\n    return self.w_down(swiglu_fused(gate, up))\n</code></pre>"},{"location":"api/kernels/activations/#implementation-notes","title":"Implementation Notes","text":""},{"location":"api/kernels/activations/#kernel-fusion","title":"Kernel Fusion","text":"<p>The Triton kernel computes <code>silu(gate) * up</code> in a single pass:</p> <pre><code>@triton.jit\ndef _swiglu_kernel(Gate, Up, Out, n_elements, BLOCK_SIZE):\n    # Load gate and up\n    gate = tl.load(Gate + offsets, mask=mask)\n    up = tl.load(Up + offsets, mask=mask)\n\n    # Fused SiLU + multiply\n    sigmoid_gate = tl.sigmoid(gate)\n    silu_gate = gate * sigmoid_gate\n    out = silu_gate * up\n\n    tl.store(Out + offsets, out, mask=mask)\n</code></pre> <p>This saves one memory round-trip compared to the separate PyTorch operations.</p>"},{"location":"api/kernels/activations/#numerical-stability","title":"Numerical Stability","text":"<p>The kernel uses the standard sigmoid implementation. For very large negative values, sigmoid approaches 0, making the output approach 0 as well (which is the correct behavior).</p>"},{"location":"api/kernels/activations/#why-swiglu","title":"Why SwiGLU?","text":"<p>SwiGLU was shown to outperform other activation functions like ReLU and GELU in the PaLM paper. The key advantages:</p> <ol> <li>Gating mechanism: The gate controls information flow, similar to attention</li> <li>Smooth gradients: SiLU provides smooth gradients everywhere (unlike ReLU)</li> <li>Better training dynamics: Empirically leads to better model quality</li> </ol> <p>The tradeoff is more parameters (3 projection matrices instead of 2) and compute, but the quality improvements are worth it for large models.</p>"},{"location":"api/kernels/activations/#references","title":"References","text":"<ul> <li>PaLM: Scaling Language Modeling with Pathways - Introduces SwiGLU for LLMs</li> <li>GLU Variants Improve Transformer - Analysis of GLU variants</li> <li>LLaMA: Open and Efficient Foundation Language Models - Uses SwiGLU</li> </ul>"},{"location":"api/kernels/gemm/","title":"INT8 GEMM","text":"<p>W8A16 (INT8 weights, FP16 activations) matrix multiplication kernels.</p>"},{"location":"api/kernels/gemm/#overview","title":"Overview","text":"<p>INT8 GEMM enables memory-efficient inference by storing weights in 8-bit format while keeping activations in FP16. This provides:</p> <ul> <li>2x memory reduction for weight storage</li> <li>Faster inference due to reduced memory bandwidth requirements</li> <li>Minimal accuracy loss with per-channel quantization</li> </ul> <p>Computation scheme:</p> <pre><code>output = (activation_fp16 @ weight_int8.dequantize()) + bias\n</code></pre> <p>The dequantization happens in registers during the matmul, so the memory traffic reduction directly translates to speedup.</p>"},{"location":"api/kernels/gemm/#performance-characteristics","title":"Performance Characteristics","text":"Configuration FP16 GEMM INT8 GEMM Speedup Memory Saved 4096x4096 156 \u03bcs 48 \u03bcs 3.3x 16 MB 8192x8192 620 \u03bcs 189 \u03bcs 3.3x 64 MB 4096x11008 418 \u03bcs 128 \u03bcs 3.3x 43 MB"},{"location":"api/kernels/gemm/#usage-examples","title":"Usage Examples","text":""},{"location":"api/kernels/gemm/#high-level-quantizedlinear","title":"High-Level: QuantizedLinear","text":"<p>The easiest way to use INT8 inference:</p> <pre><code>import torch\nfrom rotalabs_accel import QuantizedLinear\n\n# Convert existing pretrained layer\nlinear = torch.nn.Linear(4096, 4096)\nlinear.load_state_dict(pretrained_weights)\n\n# Quantize to INT8\nqlinear = QuantizedLinear.from_linear(linear)\nqlinear = qlinear.cuda()\n\n# Use like normal\nx = torch.randn(2, 512, 4096, device=\"cuda\", dtype=torch.float16)\ny = qlinear(x)  # Output is FP16\n</code></pre>"},{"location":"api/kernels/gemm/#low-level-int8linear","title":"Low-Level: Int8Linear","text":"<p>For more control over quantization:</p> <pre><code>from rotalabs_accel import Int8Linear\n\n# Create layer\nlinear = Int8Linear(\n    in_features=4096,\n    out_features=4096,\n    bias=False,\n)\n\n# Quantize weights manually\nlinear.quantize_weights(pretrained_weight_fp16)\n\n# Forward pass\ny = linear(x)\n</code></pre>"},{"location":"api/kernels/gemm/#functional-api","title":"Functional API","text":"<p>For custom implementations:</p> <pre><code>from rotalabs_accel import int8_gemm, quantize_weight_per_channel\n\n# Quantize weights once\nweight_int8, scales = quantize_weight_per_channel(weight_fp16)\n\n# Use in forward pass\noutput = int8_gemm(x, weight_int8, scales)\n</code></pre>"},{"location":"api/kernels/gemm/#quantizing-an-entire-model","title":"Quantizing an Entire Model","text":"<pre><code>from rotalabs_accel import QuantizedLinear\n\ndef quantize_model(model):\n    \"\"\"Replace all Linear layers with QuantizedLinear.\"\"\"\n    for name, module in model.named_children():\n        if isinstance(module, torch.nn.Linear):\n            quantized = QuantizedLinear.from_linear(module)\n            setattr(model, name, quantized)\n        else:\n            quantize_model(module)\n    return model\n\n# Quantize LLM\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\nmodel = quantize_model(model)\nmodel = model.cuda()\n\n# Memory usage: 14GB -&gt; 7GB (approximately)\n</code></pre>"},{"location":"api/kernels/gemm/#api-reference","title":"API Reference","text":""},{"location":"api/kernels/gemm/#functions","title":"Functions","text":""},{"location":"api/kernels/gemm/#rotalabs_accel.kernels.gemm.int8_gemm","title":"int8_gemm","text":"<pre><code>int8_gemm(\n    x: Tensor,\n    weight_int8: Tensor,\n    scale: Tensor,\n    bias: Optional[Tensor] = None,\n    weight_transposed: Optional[Tensor] = None,\n    scale_fp16: Optional[Tensor] = None,\n    use_cublas: Optional[bool] = None,\n) -&gt; torch.Tensor\n</code></pre> <p>W8A16 GEMM: FP16 activations x INT8 weights.</p> <p>Computes: y = x @ (weight_int8 * scale) + bias</p> <p>Uses Triton kernel on CUDA when available, otherwise falls back to PyTorch.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>FP16 activation tensor of shape (..., K).</p> required <code>weight_int8</code> <code>Tensor</code> <p>INT8 weight tensor of shape (N, K).</p> required <code>scale</code> <code>Tensor</code> <p>FP32 scale tensor of shape (N,) for per-output-channel dequant.</p> required <code>bias</code> <code>Optional[Tensor]</code> <p>Optional FP16 bias of shape (N,).</p> <code>None</code> <code>weight_transposed</code> <code>Optional[Tensor]</code> <p>Optional pre-transposed weight (K, N).</p> <code>None</code> <code>scale_fp16</code> <code>Optional[Tensor]</code> <p>Optional pre-converted FP16 scale (unused in Triton path).</p> <code>None</code> <code>use_cublas</code> <code>Optional[bool]</code> <p>Unused, kept for API compatibility.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>FP16 output tensor of shape (..., N).</p> Example <p>x = torch.randn(2, 8, 64) weight_int8 = torch.randint(-128, 127, (128, 64), dtype=torch.int8) scale = torch.ones(128) y = int8_gemm(x, weight_int8, scale)</p> Source code in <code>src/rotalabs_accel/kernels/gemm.py</code> <pre><code>def int8_gemm(\n    x: torch.Tensor,\n    weight_int8: torch.Tensor,\n    scale: torch.Tensor,\n    bias: Optional[torch.Tensor] = None,\n    weight_transposed: Optional[torch.Tensor] = None,\n    scale_fp16: Optional[torch.Tensor] = None,\n    use_cublas: Optional[bool] = None,\n) -&gt; torch.Tensor:\n    \"\"\"\n    W8A16 GEMM: FP16 activations x INT8 weights.\n\n    Computes: y = x @ (weight_int8 * scale) + bias\n\n    Uses Triton kernel on CUDA when available, otherwise falls back to PyTorch.\n\n    Args:\n        x: FP16 activation tensor of shape (..., K).\n        weight_int8: INT8 weight tensor of shape (N, K).\n        scale: FP32 scale tensor of shape (N,) for per-output-channel dequant.\n        bias: Optional FP16 bias of shape (N,).\n        weight_transposed: Optional pre-transposed weight (K, N).\n        scale_fp16: Optional pre-converted FP16 scale (unused in Triton path).\n        use_cublas: Unused, kept for API compatibility.\n\n    Returns:\n        FP16 output tensor of shape (..., N).\n\n    Example:\n        &gt;&gt;&gt; x = torch.randn(2, 8, 64)\n        &gt;&gt;&gt; weight_int8 = torch.randint(-128, 127, (128, 64), dtype=torch.int8)\n        &gt;&gt;&gt; scale = torch.ones(128)\n        &gt;&gt;&gt; y = int8_gemm(x, weight_int8, scale)\n    \"\"\"\n    assert weight_int8.dtype == torch.int8, \"Weight must be INT8\"\n\n    # Get dimensions\n    original_shape = x.shape\n    K = x.shape[-1]\n    N = weight_int8.shape[0]\n\n    assert weight_int8.shape == (N, K), f\"Weight shape mismatch: {weight_int8.shape} vs ({N}, {K})\"\n    assert scale.shape == (N,), f\"Scale shape mismatch: {scale.shape} vs ({N},)\"\n\n    # Use Triton kernel if available and on CUDA\n    if HAS_TRITON and x.is_cuda and weight_int8.is_cuda and scale.is_cuda:\n        return _int8_gemm_triton(x, weight_int8, scale, bias, weight_transposed)\n\n    # Fallback to PyTorch\n    return int8_gemm_torch(x, weight_int8, scale, bias)\n</code></pre>"},{"location":"api/kernels/gemm/#rotalabs_accel.kernels.gemm.int8_gemm_torch","title":"int8_gemm_torch","text":"<pre><code>int8_gemm_torch(\n    x: Tensor,\n    weight_int8: Tensor,\n    scale: Tensor,\n    bias: Optional[Tensor] = None,\n) -&gt; torch.Tensor\n</code></pre> <p>PyTorch reference implementation of W8A16 GEMM.</p> <p>Works on any device (CPU or CUDA).</p> Source code in <code>src/rotalabs_accel/kernels/gemm.py</code> <pre><code>def int8_gemm_torch(\n    x: torch.Tensor,\n    weight_int8: torch.Tensor,\n    scale: torch.Tensor,\n    bias: Optional[torch.Tensor] = None,\n) -&gt; torch.Tensor:\n    \"\"\"\n    PyTorch reference implementation of W8A16 GEMM.\n\n    Works on any device (CPU or CUDA).\n    \"\"\"\n    # Dequantize weight to same dtype as input\n    weight_dequant = dequantize(weight_int8, scale, dtype=x.dtype, dim=0)\n\n    # Standard matmul\n    y = torch.nn.functional.linear(x, weight_dequant, bias)\n\n    return y\n</code></pre>"},{"location":"api/kernels/gemm/#classes","title":"Classes","text":""},{"location":"api/kernels/gemm/#rotalabs_accel.kernels.gemm.Int8Linear","title":"Int8Linear","text":"<p>               Bases: <code>Module</code></p> <p>Linear layer using INT8 weights with optimized GEMM kernel.</p> <p>Uses Triton kernel on CUDA when available, otherwise falls back to PyTorch.</p> <p>Parameters:</p> Name Type Description Default <code>in_features</code> <code>int</code> <p>Input dimension (K).</p> required <code>out_features</code> <code>int</code> <p>Output dimension (N).</p> required <code>bias</code> <code>bool</code> <p>Whether to include bias.</p> <code>False</code> Example <p>linear = Int8Linear(64, 128) linear.quantize_weights(torch.randn(128, 64)) y = linear(torch.randn(2, 8, 64))</p> Source code in <code>src/rotalabs_accel/kernels/gemm.py</code> <pre><code>class Int8Linear(torch.nn.Module):\n    \"\"\"\n    Linear layer using INT8 weights with optimized GEMM kernel.\n\n    Uses Triton kernel on CUDA when available, otherwise falls back to PyTorch.\n\n    Args:\n        in_features: Input dimension (K).\n        out_features: Output dimension (N).\n        bias: Whether to include bias.\n\n    Example:\n        &gt;&gt;&gt; linear = Int8Linear(64, 128)\n        &gt;&gt;&gt; linear.quantize_weights(torch.randn(128, 64))\n        &gt;&gt;&gt; y = linear(torch.randn(2, 8, 64))\n    \"\"\"\n\n    def __init__(\n        self,\n        in_features: int,\n        out_features: int,\n        bias: bool = False,\n    ):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n\n        # INT8 weights: (out_features, in_features)\n        self.register_buffer(\n            \"weight_int8\",\n            torch.zeros(out_features, in_features, dtype=torch.int8)\n        )\n        self.register_buffer(\n            \"scale\",\n            torch.ones(out_features, dtype=torch.float32)\n        )\n        # Pre-transposed weight for Triton path\n        self.register_buffer(\n            \"weight_transposed\",\n            torch.zeros(in_features, out_features, dtype=torch.int8)\n        )\n\n        if bias:\n            self.bias = torch.nn.Parameter(torch.zeros(out_features, dtype=torch.float16))\n        else:\n            self.register_parameter(\"bias\", None)\n\n        self._quantized = False\n\n    def quantize_weights(self, weight: torch.Tensor) -&gt; None:\n        \"\"\"Quantize and store FP16/FP32 weights as INT8.\"\"\"\n        assert weight.shape == (self.out_features, self.in_features)\n\n        # Quantization happens on CPU\n        weight_cpu = weight.cpu() if weight.is_cuda else weight\n        weight_int8, scale = quantize_weight_per_channel(weight_cpu)\n\n        # Move to same device as buffers and copy\n        device = self.weight_int8.device\n        self.weight_int8.copy_(weight_int8.to(device))\n        self.scale.copy_(scale.to(device))\n        self.weight_transposed.copy_(weight_int8.t().contiguous().to(device))\n        self._quantized = True\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass using optimized INT8 GEMM kernel.\"\"\"\n        if not self._quantized:\n            raise RuntimeError(\"Weights not quantized. Call quantize_weights() first.\")\n\n        return int8_gemm(\n            x, self.weight_int8, self.scale, self.bias,\n            weight_transposed=self.weight_transposed,\n        )\n\n    @classmethod\n    def from_linear(cls, linear: torch.nn.Linear) -&gt; \"Int8Linear\":\n        \"\"\"Convert nn.Linear to Int8Linear.\"\"\"\n        has_bias = linear.bias is not None\n        int8_linear = cls(linear.in_features, linear.out_features, bias=has_bias)\n\n        # Move to same device as input linear, then quantize\n        device = linear.weight.device\n        int8_linear = int8_linear.to(device)\n        int8_linear.quantize_weights(linear.weight.data)\n\n        # Copy bias if present\n        if has_bias:\n            int8_linear.bias.data.copy_(linear.bias.data.half())\n\n        return int8_linear\n\n    def extra_repr(self) -&gt; str:\n        return (\n            f\"in_features={self.in_features}, out_features={self.out_features}, \"\n            f\"bias={self.bias is not None}, quantized={self._quantized}\"\n        )\n</code></pre>"},{"location":"api/kernels/gemm/#rotalabs_accel.kernels.gemm.Int8Linear.__init__","title":"__init__","text":"<pre><code>__init__(\n    in_features: int, out_features: int, bias: bool = False\n)\n</code></pre> Source code in <code>src/rotalabs_accel/kernels/gemm.py</code> <pre><code>def __init__(\n    self,\n    in_features: int,\n    out_features: int,\n    bias: bool = False,\n):\n    super().__init__()\n    self.in_features = in_features\n    self.out_features = out_features\n\n    # INT8 weights: (out_features, in_features)\n    self.register_buffer(\n        \"weight_int8\",\n        torch.zeros(out_features, in_features, dtype=torch.int8)\n    )\n    self.register_buffer(\n        \"scale\",\n        torch.ones(out_features, dtype=torch.float32)\n    )\n    # Pre-transposed weight for Triton path\n    self.register_buffer(\n        \"weight_transposed\",\n        torch.zeros(in_features, out_features, dtype=torch.int8)\n    )\n\n    if bias:\n        self.bias = torch.nn.Parameter(torch.zeros(out_features, dtype=torch.float16))\n    else:\n        self.register_parameter(\"bias\", None)\n\n    self._quantized = False\n</code></pre>"},{"location":"api/kernels/gemm/#rotalabs_accel.kernels.gemm.Int8Linear.quantize_weights","title":"quantize_weights","text":"<pre><code>quantize_weights(weight: Tensor) -&gt; None\n</code></pre> <p>Quantize and store FP16/FP32 weights as INT8.</p> Source code in <code>src/rotalabs_accel/kernels/gemm.py</code> <pre><code>def quantize_weights(self, weight: torch.Tensor) -&gt; None:\n    \"\"\"Quantize and store FP16/FP32 weights as INT8.\"\"\"\n    assert weight.shape == (self.out_features, self.in_features)\n\n    # Quantization happens on CPU\n    weight_cpu = weight.cpu() if weight.is_cuda else weight\n    weight_int8, scale = quantize_weight_per_channel(weight_cpu)\n\n    # Move to same device as buffers and copy\n    device = self.weight_int8.device\n    self.weight_int8.copy_(weight_int8.to(device))\n    self.scale.copy_(scale.to(device))\n    self.weight_transposed.copy_(weight_int8.t().contiguous().to(device))\n    self._quantized = True\n</code></pre>"},{"location":"api/kernels/gemm/#rotalabs_accel.kernels.gemm.Int8Linear.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Forward pass using optimized INT8 GEMM kernel.</p> Source code in <code>src/rotalabs_accel/kernels/gemm.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass using optimized INT8 GEMM kernel.\"\"\"\n    if not self._quantized:\n        raise RuntimeError(\"Weights not quantized. Call quantize_weights() first.\")\n\n    return int8_gemm(\n        x, self.weight_int8, self.scale, self.bias,\n        weight_transposed=self.weight_transposed,\n    )\n</code></pre>"},{"location":"api/kernels/gemm/#quantization-details","title":"Quantization Details","text":""},{"location":"api/kernels/gemm/#symmetric-quantization","title":"Symmetric Quantization","text":"<p>We use symmetric quantization with per-channel scales:</p> <pre><code>scale[i] = max(|weight[i, :]|) / 127\nweight_int8[i, :] = round(weight[i, :] / scale[i]).clamp(-128, 127)\n</code></pre> <p>This provides a good balance between accuracy and performance.</p>"},{"location":"api/kernels/gemm/#per-channel-vs-per-tensor","title":"Per-Channel vs Per-Tensor","text":"Method Accuracy Memory Speed Per-tensor Lower Minimal overhead Fastest Per-channel Higher 1 scale per output Slightly slower <p>We default to per-channel (per-output-row) quantization for better accuracy.</p>"},{"location":"api/kernels/gemm/#quantization-error","title":"Quantization Error","text":"<p>Typical quantization errors for random weights:</p> Metric Value Max absolute error ~0.01 Mean absolute error ~0.002 SNR ~45-50 dB <p>For real model weights, errors may vary based on weight distribution.</p>"},{"location":"api/kernels/gemm/#implementation-notes","title":"Implementation Notes","text":""},{"location":"api/kernels/gemm/#memory-layout","title":"Memory Layout","text":"<ul> <li>Weights: INT8, shape <code>(out_features, in_features)</code></li> <li>Scales: FP32, shape <code>(out_features,)</code></li> <li>Activations: FP16, shape <code>(..., in_features)</code></li> <li>Output: FP16, shape <code>(..., out_features)</code></li> </ul>"},{"location":"api/kernels/gemm/#kernel-strategy","title":"Kernel Strategy","text":"<p>The Triton kernel:</p> <ol> <li>Loads weight tiles as INT8 (1 byte per element)</li> <li>Dequantizes in registers using the scale vector</li> <li>Performs FP16 matmul with FP32 accumulation</li> <li>Stores FP16 output</li> </ol> <p>This minimizes memory traffic while maintaining numerical precision.</p>"},{"location":"api/kernels/gemm/#fallback-behavior","title":"Fallback Behavior","text":"<p>On CPU or without Triton, the kernel falls back to:</p> <pre><code>def int8_gemm_torch(x, weight_int8, scale):\n    weight_fp = (weight_int8.float() * scale.unsqueeze(1)).to(x.dtype)\n    return x @ weight_fp.T\n</code></pre>"},{"location":"api/kernels/gemm/#comparison-with-other-quantization-methods","title":"Comparison with Other Quantization Methods","text":"Method Bits Scheme Accuracy Speed INT8 (this) 8 Symmetric High Fast GPTQ 4 Asymmetric + groups Medium-High Moderate AWQ 4 Activation-aware High Moderate FP8 (Hopper) 8 Native hardware Very High Very Fast <p>INT8 symmetric quantization is a good default choice that works on all GPUs and provides a solid accuracy/speed tradeoff.</p>"},{"location":"api/kernels/gemm/#references","title":"References","text":"<ul> <li>LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale</li> <li>SmoothQuant: Accurate and Efficient Post-Training Quantization for LLMs</li> <li>GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers</li> </ul>"},{"location":"api/kernels/normalization/","title":"Normalization","text":"<p>RMSNorm (Root Mean Square Layer Normalization) kernels with optional residual fusion.</p>"},{"location":"api/kernels/normalization/#overview","title":"Overview","text":"<p>RMSNorm is a simpler alternative to LayerNorm used in modern LLMs like LLaMA, Mistral, and Qwen. Unlike LayerNorm, it doesn't center the input (no mean subtraction), which reduces computation.</p> <p>Mathematical formula:</p> \\[ \\text{RMSNorm}(x) = \\frac{x}{\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} x_i^2 + \\epsilon}} \\cdot \\gamma \\] <p>Where:</p> <ul> <li>\\(x\\) is the input tensor</li> <li>\\(n\\) is the hidden dimension</li> <li>\\(\\epsilon\\) is a small constant for numerical stability (typically 1e-6)</li> <li>\\(\\gamma\\) is the learnable weight parameter</li> </ul>"},{"location":"api/kernels/normalization/#performance-characteristics","title":"Performance Characteristics","text":"<p>RMSNorm is a memory-bound operation with low arithmetic intensity (~1-2 FLOPs/byte). The Triton kernel provides speedups by:</p> <ol> <li>Fusing operations: Combines variance computation, normalization, and scaling in a single kernel</li> <li>Reducing memory traffic: Reads input once, writes output once</li> <li>Using FP32 accumulation: Ensures numerical stability while keeping I/O in FP16</li> </ol> Configuration PyTorch Triton Speedup hidden=4096, seq=2048 45 \u03bcs 12 \u03bcs 3.8x hidden=8192, seq=2048 89 \u03bcs 24 \u03bcs 3.7x hidden=4096, seq=8192 178 \u03bcs 47 \u03bcs 3.8x"},{"location":"api/kernels/normalization/#usage-examples","title":"Usage Examples","text":""},{"location":"api/kernels/normalization/#basic-rmsnorm","title":"Basic RMSNorm","text":"<pre><code>import torch\nfrom rotalabs_accel import TritonRMSNorm, rmsnorm\n\n# Module API (recommended for models)\nnorm = TritonRMSNorm(hidden_size=4096, eps=1e-6)\nx = torch.randn(2, 512, 4096, device=\"cuda\", dtype=torch.float16)\ny = norm(x)\n\n# Functional API (for custom implementations)\nweight = torch.ones(4096, device=\"cuda\", dtype=torch.float16)\ny = rmsnorm(x, weight, eps=1e-6)\n</code></pre>"},{"location":"api/kernels/normalization/#fused-residual-rmsnorm","title":"Fused Residual + RMSNorm","text":"<p>In transformer blocks, RMSNorm is typically applied after adding a residual:</p> <pre><code>from rotalabs_accel import rmsnorm_residual_fused\n\n# Standard pattern (two operations):\n# x = x + residual\n# x = rmsnorm(x, weight)\n\n# Fused version (one operation, ~2x faster):\nx = rmsnorm_residual_fused(x, residual, weight, eps=1e-6)\n</code></pre> <p>The fused version eliminates an intermediate tensor allocation and memory round-trip.</p>"},{"location":"api/kernels/normalization/#api-reference","title":"API Reference","text":""},{"location":"api/kernels/normalization/#functions","title":"Functions","text":""},{"location":"api/kernels/normalization/#rotalabs_accel.kernels.normalization.rmsnorm","title":"rmsnorm","text":"<pre><code>rmsnorm(\n    x: Tensor, weight: Tensor, eps: float = 1e-06\n) -&gt; torch.Tensor\n</code></pre> <p>Apply RMS normalization to input tensor.</p> <p>Uses Triton kernel on CUDA when available, otherwise falls back to PyTorch.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (..., hidden_dim).</p> required <code>weight</code> <code>Tensor</code> <p>Learnable scale parameter of shape (hidden_dim,).</p> required <code>eps</code> <code>float</code> <p>Small constant for numerical stability.</p> <code>1e-06</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Normalized tensor of same shape as input.</p> Example <p>x = torch.randn(2, 8, 64) weight = torch.ones(64) y = rmsnorm(x, weight, eps=1e-6)</p> Source code in <code>src/rotalabs_accel/kernels/normalization.py</code> <pre><code>def rmsnorm(\n    x: torch.Tensor,\n    weight: torch.Tensor,\n    eps: float = 1e-6,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Apply RMS normalization to input tensor.\n\n    Uses Triton kernel on CUDA when available, otherwise falls back to PyTorch.\n\n    Args:\n        x: Input tensor of shape (..., hidden_dim).\n        weight: Learnable scale parameter of shape (hidden_dim,).\n        eps: Small constant for numerical stability.\n\n    Returns:\n        Normalized tensor of same shape as input.\n\n    Example:\n        &gt;&gt;&gt; x = torch.randn(2, 8, 64)\n        &gt;&gt;&gt; weight = torch.ones(64)\n        &gt;&gt;&gt; y = rmsnorm(x, weight, eps=1e-6)\n    \"\"\"\n    assert x.shape[-1] == weight.shape[0], f\"Hidden dim mismatch: {x.shape[-1]} vs {weight.shape[0]}\"\n\n    # Use Triton kernel if available and on CUDA\n    if HAS_TRITON and x.is_cuda and weight.is_cuda:\n        return _rmsnorm_triton(x, weight, eps)\n\n    # Fallback to PyTorch\n    return rmsnorm_torch(x, weight, eps)\n</code></pre>"},{"location":"api/kernels/normalization/#rotalabs_accel.kernels.normalization.rmsnorm_torch","title":"rmsnorm_torch","text":"<pre><code>rmsnorm_torch(\n    x: Tensor, weight: Tensor, eps: float = 1e-06\n) -&gt; torch.Tensor\n</code></pre> <p>PyTorch reference implementation of RMSNorm.</p> <p>Works on any device (CPU or CUDA).</p> Source code in <code>src/rotalabs_accel/kernels/normalization.py</code> <pre><code>def rmsnorm_torch(\n    x: torch.Tensor,\n    weight: torch.Tensor,\n    eps: float = 1e-6,\n) -&gt; torch.Tensor:\n    \"\"\"\n    PyTorch reference implementation of RMSNorm.\n\n    Works on any device (CPU or CUDA).\n    \"\"\"\n    # Compute in FP32 for numerical stability\n    x_fp32 = x.float()\n    rms = torch.rsqrt(x_fp32.pow(2).mean(dim=-1, keepdim=True) + eps)\n    return (x_fp32 * rms).to(x.dtype) * weight\n</code></pre>"},{"location":"api/kernels/normalization/#rotalabs_accel.kernels.normalization.rmsnorm_residual_fused","title":"rmsnorm_residual_fused","text":"<pre><code>rmsnorm_residual_fused(\n    x: Tensor,\n    residual: Tensor,\n    weight: Tensor,\n    eps: float = 1e-06,\n) -&gt; torch.Tensor\n</code></pre> <p>Fused RMSNorm with residual addition.</p> <p>Computes: y = RMSNorm(x + residual) * weight</p> <p>Uses Triton kernel on CUDA when available, otherwise falls back to PyTorch.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (..., hidden_dim).</p> required <code>residual</code> <code>Tensor</code> <p>Residual tensor of same shape as x.</p> required <code>weight</code> <code>Tensor</code> <p>Learnable scale parameter of shape (hidden_dim,).</p> required <code>eps</code> <code>float</code> <p>Small constant for numerical stability.</p> <code>1e-06</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Normalized tensor of same shape as input.</p> Example <p>x = torch.randn(2, 8, 64) residual = torch.randn_like(x) weight = torch.ones(64) y = rmsnorm_residual_fused(x, residual, weight, eps=1e-6)</p> Source code in <code>src/rotalabs_accel/kernels/normalization.py</code> <pre><code>def rmsnorm_residual_fused(\n    x: torch.Tensor,\n    residual: torch.Tensor,\n    weight: torch.Tensor,\n    eps: float = 1e-6,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Fused RMSNorm with residual addition.\n\n    Computes: y = RMSNorm(x + residual) * weight\n\n    Uses Triton kernel on CUDA when available, otherwise falls back to PyTorch.\n\n    Args:\n        x: Input tensor of shape (..., hidden_dim).\n        residual: Residual tensor of same shape as x.\n        weight: Learnable scale parameter of shape (hidden_dim,).\n        eps: Small constant for numerical stability.\n\n    Returns:\n        Normalized tensor of same shape as input.\n\n    Example:\n        &gt;&gt;&gt; x = torch.randn(2, 8, 64)\n        &gt;&gt;&gt; residual = torch.randn_like(x)\n        &gt;&gt;&gt; weight = torch.ones(64)\n        &gt;&gt;&gt; y = rmsnorm_residual_fused(x, residual, weight, eps=1e-6)\n    \"\"\"\n    assert x.shape == residual.shape, f\"Shape mismatch: x={x.shape}, residual={residual.shape}\"\n    assert x.shape[-1] == weight.shape[0], f\"Hidden dim mismatch: {x.shape[-1]} vs {weight.shape[0]}\"\n\n    # Use Triton kernel if available and on CUDA\n    if HAS_TRITON and x.is_cuda and residual.is_cuda and weight.is_cuda:\n        return _rmsnorm_residual_triton(x, residual, weight, eps)\n\n    # Fallback to PyTorch\n    return rmsnorm_residual_torch(x, residual, weight, eps)\n</code></pre>"},{"location":"api/kernels/normalization/#classes","title":"Classes","text":""},{"location":"api/kernels/normalization/#rotalabs_accel.kernels.normalization.TritonRMSNorm","title":"TritonRMSNorm","text":"<p>               Bases: <code>Module</code></p> <p>RMSNorm layer with automatic Triton/PyTorch dispatch.</p> <p>Drop-in replacement for torch.nn.RMSNorm with identical interface. Uses Triton kernel on CUDA when available, otherwise falls back to PyTorch.</p> <p>Parameters:</p> Name Type Description Default <code>hidden_size</code> <code>int</code> <p>Size of the last dimension to normalize over.</p> required <code>eps</code> <code>float</code> <p>Small constant for numerical stability.</p> <code>1e-06</code> Example <p>norm = TritonRMSNorm(64) x = torch.randn(2, 8, 64) y = norm(x)</p> Source code in <code>src/rotalabs_accel/kernels/normalization.py</code> <pre><code>class TritonRMSNorm(torch.nn.Module):\n    \"\"\"\n    RMSNorm layer with automatic Triton/PyTorch dispatch.\n\n    Drop-in replacement for torch.nn.RMSNorm with identical interface.\n    Uses Triton kernel on CUDA when available, otherwise falls back to PyTorch.\n\n    Args:\n        hidden_size: Size of the last dimension to normalize over.\n        eps: Small constant for numerical stability.\n\n    Example:\n        &gt;&gt;&gt; norm = TritonRMSNorm(64)\n        &gt;&gt;&gt; x = torch.randn(2, 8, 64)\n        &gt;&gt;&gt; y = norm(x)\n    \"\"\"\n\n    def __init__(self, hidden_size: int, eps: float = 1e-6):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.eps = eps\n        self.weight = torch.nn.Parameter(torch.ones(hidden_size))\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        return rmsnorm(x, self.weight, self.eps)\n\n    def extra_repr(self) -&gt; str:\n        return f\"{self.hidden_size}, eps={self.eps}\"\n</code></pre>"},{"location":"api/kernels/normalization/#rotalabs_accel.kernels.normalization.TritonRMSNorm.__init__","title":"__init__","text":"<pre><code>__init__(hidden_size: int, eps: float = 1e-06)\n</code></pre> Source code in <code>src/rotalabs_accel/kernels/normalization.py</code> <pre><code>def __init__(self, hidden_size: int, eps: float = 1e-6):\n    super().__init__()\n    self.hidden_size = hidden_size\n    self.eps = eps\n    self.weight = torch.nn.Parameter(torch.ones(hidden_size))\n</code></pre>"},{"location":"api/kernels/normalization/#rotalabs_accel.kernels.normalization.TritonRMSNorm.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; torch.Tensor\n</code></pre> Source code in <code>src/rotalabs_accel/kernels/normalization.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    return rmsnorm(x, self.weight, self.eps)\n</code></pre>"},{"location":"api/kernels/normalization/#implementation-notes","title":"Implementation Notes","text":""},{"location":"api/kernels/normalization/#automatic-dispatch","title":"Automatic Dispatch","text":"<p>The <code>rmsnorm</code> function automatically selects the best implementation:</p> <pre><code>def rmsnorm(x, weight, eps=1e-6):\n    if HAS_TRITON and x.is_cuda and weight.is_cuda:\n        return _rmsnorm_triton(x, weight, eps)\n    return rmsnorm_torch(x, weight, eps)\n</code></pre>"},{"location":"api/kernels/normalization/#numerical-stability","title":"Numerical Stability","text":"<p>All implementations use FP32 accumulation for the variance computation, even when inputs are FP16/BF16. This prevents numerical issues with large hidden dimensions.</p>"},{"location":"api/kernels/normalization/#block-size-selection","title":"Block Size Selection","text":"<p>The Triton kernel automatically selects block sizes based on the hidden dimension:</p> <pre><code>BLOCK_SIZE = min(triton.next_power_of_2(hidden_dim), 8192)\n</code></pre>"},{"location":"api/kernels/normalization/#references","title":"References","text":"<ul> <li>Root Mean Square Layer Normalization - Original RMSNorm paper</li> <li>LLaMA: Open and Efficient Foundation Language Models - Uses RMSNorm</li> </ul>"},{"location":"api/kernels/rope/","title":"Rotary Position Embeddings (RoPE)","text":"<p>Rotary Position Embeddings for encoding position information in attention layers.</p>"},{"location":"api/kernels/rope/#overview","title":"Overview","text":"<p>RoPE encodes position information by rotating query and key vectors in 2D subspaces. It's used in LLaMA, Mistral, Qwen, and most modern LLMs.</p> <p>Mathematical formula:</p> <p>For a vector \\(x\\) at position \\(m\\), the rotated vector is:</p> \\[ \\text{RoPE}(x, m) = \\begin{pmatrix} x_0 \\\\ x_1 \\\\ x_2 \\\\ x_3 \\\\ \\vdots \\end{pmatrix} \\odot \\begin{pmatrix} \\cos(m\\theta_0) \\\\ \\cos(m\\theta_0) \\\\ \\cos(m\\theta_1) \\\\ \\cos(m\\theta_1) \\\\ \\vdots \\end{pmatrix} + \\begin{pmatrix} -x_1 \\\\ x_0 \\\\ -x_3 \\\\ x_2 \\\\ \\vdots \\end{pmatrix} \\odot \\begin{pmatrix} \\sin(m\\theta_0) \\\\ \\sin(m\\theta_0) \\\\ \\sin(m\\theta_1) \\\\ \\sin(m\\theta_1) \\\\ \\vdots \\end{pmatrix} \\] <p>Where \\(\\theta_i = \\frac{1}{\\text{base}^{2i/d}}\\) with typical base=10000.</p>"},{"location":"api/kernels/rope/#key-properties","title":"Key Properties","text":"<ol> <li>Relative position encoding: The dot product between rotated vectors depends only on their relative position</li> <li>Long-range decay: Attention naturally decays with distance due to the rotation frequencies</li> <li>No learned parameters: Position encodings are computed, not learned</li> </ol>"},{"location":"api/kernels/rope/#performance-characteristics","title":"Performance Characteristics","text":"Configuration PyTorch Triton Speedup head_dim=128, seq=2048 67 \u03bcs 23 \u03bcs 2.9x head_dim=128, seq=8192 267 \u03bcs 92 \u03bcs 2.9x head_dim=64, seq=2048 34 \u03bcs 12 \u03bcs 2.8x"},{"location":"api/kernels/rope/#usage-examples","title":"Usage Examples","text":""},{"location":"api/kernels/rope/#module-api-recommended","title":"Module API (Recommended)","text":"<pre><code>import torch\nfrom rotalabs_accel import RotaryEmbedding\n\n# Create RoPE module\nrope = RotaryEmbedding(\n    dim=128,           # Head dimension\n    max_seq_len=8192,  # Maximum sequence length\n    base=10000.0,      # Frequency base (standard is 10000)\n)\n\n# Query and Key tensors\n# Shape: [batch, seq_len, num_heads, head_dim]\nq = torch.randn(2, 512, 32, 128, device=\"cuda\", dtype=torch.float16)\nk = torch.randn(2, 512, 32, 128, device=\"cuda\", dtype=torch.float16)\n\n# Apply RoPE\nq_rot, k_rot = rope(q, k, seq_len=512)\n</code></pre>"},{"location":"api/kernels/rope/#functional-api","title":"Functional API","text":"<pre><code>from rotalabs_accel import build_rope_cache, apply_rope\n\n# Build cache once (at model initialization)\ncos, sin = build_rope_cache(\n    seq_len=8192,\n    head_dim=128,\n    base=10000.0,\n    device=\"cuda\",\n)\n\n# Apply during forward pass\n# Slice cache to actual sequence length\nq_rot, k_rot = apply_rope(q, k, cos[:seq_len], sin[:seq_len])\n</code></pre>"},{"location":"api/kernels/rope/#with-grouped-query-attention-gqa","title":"With Grouped Query Attention (GQA)","text":"<p>RoPE works with different numbers of Q and K heads:</p> <pre><code># LLaMA 3 style: 32 Q heads, 8 KV heads\nq = torch.randn(2, 512, 32, 128, device=\"cuda\")  # 32 heads\nk = torch.randn(2, 512, 8, 128, device=\"cuda\")   # 8 heads\n\n# apply_rope handles broadcasting automatically\nq_rot, k_rot = rope(q, k, seq_len=512)\n</code></pre>"},{"location":"api/kernels/rope/#position-offset-for-kv-cache","title":"Position Offset (for KV Cache)","text":"<p>During generation with KV cache, you need to offset positions:</p> <pre><code># First token: position 0\nq1, k1 = rope(q[:, :1], k[:, :1], seq_len=1)\ncached_k = k1\n\n# Next token: position 1\n# Pass offset to start from correct position\nq2, k2 = rope(q[:, :1], k[:, :1], seq_len=1, offset=1)\ncached_k = torch.cat([cached_k, k2], dim=1)\n</code></pre>"},{"location":"api/kernels/rope/#api-reference","title":"API Reference","text":""},{"location":"api/kernels/rope/#functions","title":"Functions","text":""},{"location":"api/kernels/rope/#rotalabs_accel.kernels.rope.apply_rope","title":"apply_rope","text":"<pre><code>apply_rope(\n    q: Tensor,\n    k: Tensor,\n    cos: Tensor,\n    sin: Tensor,\n    use_triton: Optional[bool] = None,\n) -&gt; Tuple[torch.Tensor, torch.Tensor]\n</code></pre> <p>Apply Rotary Position Embeddings to query and key tensors.</p> <p>Uses Triton kernel on CUDA when available, otherwise falls back to PyTorch.</p> <p>Parameters:</p> Name Type Description Default <code>q</code> <code>Tensor</code> <p>Query tensor [batch, seq, heads, head_dim]</p> required <code>k</code> <code>Tensor</code> <p>Key tensor [batch, seq, heads, head_dim]</p> required <code>cos</code> <code>Tensor</code> <p>Cosine cache for positions</p> required <code>sin</code> <code>Tensor</code> <p>Sine cache for positions</p> required <code>use_triton</code> <code>Optional[bool]</code> <p>Force Triton (True) or PyTorch (False). None = auto.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>Tuple of (q_rotated, k_rotated) with same shapes as inputs.</p> Example <p>q = torch.randn(2, 16, 4, 32) k = torch.randn(2, 16, 4, 32) cos, sin = build_rope_cache(16, 32) q_rot, k_rot = apply_rope(q, k, cos, sin)</p> Source code in <code>src/rotalabs_accel/kernels/rope.py</code> <pre><code>def apply_rope(\n    q: torch.Tensor,\n    k: torch.Tensor,\n    cos: torch.Tensor,\n    sin: torch.Tensor,\n    use_triton: Optional[bool] = None,\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Apply Rotary Position Embeddings to query and key tensors.\n\n    Uses Triton kernel on CUDA when available, otherwise falls back to PyTorch.\n\n    Args:\n        q: Query tensor [batch, seq, heads, head_dim]\n        k: Key tensor [batch, seq, heads, head_dim]\n        cos: Cosine cache for positions\n        sin: Sine cache for positions\n        use_triton: Force Triton (True) or PyTorch (False). None = auto.\n\n    Returns:\n        Tuple of (q_rotated, k_rotated) with same shapes as inputs.\n\n    Example:\n        &gt;&gt;&gt; q = torch.randn(2, 16, 4, 32)\n        &gt;&gt;&gt; k = torch.randn(2, 16, 4, 32)\n        &gt;&gt;&gt; cos, sin = build_rope_cache(16, 32)\n        &gt;&gt;&gt; q_rot, k_rot = apply_rope(q, k, cos, sin)\n    \"\"\"\n    if use_triton is None:\n        use_triton = HAS_TRITON and q.is_cuda\n\n    if use_triton and HAS_TRITON:\n        return _rope_triton(q, k, cos, sin)\n    else:\n        return rope_torch(q, k, cos, sin)\n</code></pre>"},{"location":"api/kernels/rope/#rotalabs_accel.kernels.rope.rope_torch","title":"rope_torch","text":"<pre><code>rope_torch(\n    q: Tensor, k: Tensor, cos: Tensor, sin: Tensor\n) -&gt; Tuple[torch.Tensor, torch.Tensor]\n</code></pre> <p>PyTorch reference implementation of RoPE.</p> <p>Works on any device (CPU or CUDA).</p> <p>Parameters:</p> Name Type Description Default <code>q</code> <code>Tensor</code> <p>Query tensor [batch, seq, heads, head_dim]</p> required <code>k</code> <code>Tensor</code> <p>Key tensor [batch, seq, heads, head_dim]</p> required <code>cos</code> <code>Tensor</code> <p>Cosine cache [seq, head_dim/2] or broadcastable</p> required <code>sin</code> <code>Tensor</code> <p>Sine cache [seq, head_dim/2] or broadcastable</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>Tuple of (q_rotated, k_rotated).</p> Source code in <code>src/rotalabs_accel/kernels/rope.py</code> <pre><code>def rope_torch(\n    q: torch.Tensor,\n    k: torch.Tensor,\n    cos: torch.Tensor,\n    sin: torch.Tensor,\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    PyTorch reference implementation of RoPE.\n\n    Works on any device (CPU or CUDA).\n\n    Args:\n        q: Query tensor [batch, seq, heads, head_dim]\n        k: Key tensor [batch, seq, heads, head_dim]\n        cos: Cosine cache [seq, head_dim/2] or broadcastable\n        sin: Sine cache [seq, head_dim/2] or broadcastable\n\n    Returns:\n        Tuple of (q_rotated, k_rotated).\n    \"\"\"\n    # Reshape for rotation: split last dim into pairs\n    q_reshape = q.view(*q.shape[:-1], -1, 2)  # [..., head_dim/2, 2]\n    k_reshape = k.view(*k.shape[:-1], -1, 2)\n\n    # Expand cos/sin if needed\n    if cos.dim() == 2:\n        cos = cos.unsqueeze(0).unsqueeze(2)  # [1, seq, 1, head_dim/2]\n        sin = sin.unsqueeze(0).unsqueeze(2)\n\n    # Apply rotation\n    q_rot = torch.stack([\n        q_reshape[..., 0] * cos - q_reshape[..., 1] * sin,\n        q_reshape[..., 0] * sin + q_reshape[..., 1] * cos,\n    ], dim=-1).flatten(-2)\n\n    k_rot = torch.stack([\n        k_reshape[..., 0] * cos - k_reshape[..., 1] * sin,\n        k_reshape[..., 0] * sin + k_reshape[..., 1] * cos,\n    ], dim=-1).flatten(-2)\n\n    return q_rot, k_rot\n</code></pre>"},{"location":"api/kernels/rope/#rotalabs_accel.kernels.rope.build_rope_cache","title":"build_rope_cache","text":"<pre><code>build_rope_cache(\n    seq_len: int,\n    head_dim: int,\n    base: float = 10000.0,\n    device: Optional[device] = None,\n    dtype: dtype = torch.float32,\n) -&gt; Tuple[torch.Tensor, torch.Tensor]\n</code></pre> <p>Build cosine and sine caches for RoPE.</p> <p>Parameters:</p> Name Type Description Default <code>seq_len</code> <code>int</code> <p>Maximum sequence length.</p> required <code>head_dim</code> <code>int</code> <p>Dimension of each attention head.</p> required <code>base</code> <code>float</code> <p>Base for the frequency computation (default: 10000).</p> <code>10000.0</code> <code>device</code> <code>Optional[device]</code> <p>Device for the tensors.</p> <code>None</code> <code>dtype</code> <code>dtype</code> <p>Data type for the tensors.</p> <code>float32</code> <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>Tuple of (cos_cache, sin_cache), each of shape [seq_len, head_dim/2].</p> Example <p>cos, sin = build_rope_cache(2048, 128, device='cuda') print(cos.shape)  # torch.Size([2048, 64])</p> Source code in <code>src/rotalabs_accel/kernels/rope.py</code> <pre><code>def build_rope_cache(\n    seq_len: int,\n    head_dim: int,\n    base: float = 10000.0,\n    device: Optional[torch.device] = None,\n    dtype: torch.dtype = torch.float32,\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Build cosine and sine caches for RoPE.\n\n    Args:\n        seq_len: Maximum sequence length.\n        head_dim: Dimension of each attention head.\n        base: Base for the frequency computation (default: 10000).\n        device: Device for the tensors.\n        dtype: Data type for the tensors.\n\n    Returns:\n        Tuple of (cos_cache, sin_cache), each of shape [seq_len, head_dim/2].\n\n    Example:\n        &gt;&gt;&gt; cos, sin = build_rope_cache(2048, 128, device='cuda')\n        &gt;&gt;&gt; print(cos.shape)  # torch.Size([2048, 64])\n    \"\"\"\n    assert head_dim % 2 == 0, f\"Head dim must be even, got {head_dim}\"\n\n    # Compute frequencies: theta_i = base^(-2i/d) for i in [0, d/2)\n    half_dim = head_dim // 2\n    freq_seq = torch.arange(half_dim, device=device, dtype=dtype)\n    inv_freq = 1.0 / (base ** (freq_seq / half_dim))\n\n    # Compute positions\n    positions = torch.arange(seq_len, device=device, dtype=dtype)\n\n    # Outer product: [seq_len, half_dim]\n    angles = torch.outer(positions, inv_freq)\n\n    # Compute cos and sin\n    cos_cache = torch.cos(angles)\n    sin_cache = torch.sin(angles)\n\n    return cos_cache, sin_cache\n</code></pre>"},{"location":"api/kernels/rope/#classes","title":"Classes","text":""},{"location":"api/kernels/rope/#rotalabs_accel.kernels.rope.RotaryEmbedding","title":"RotaryEmbedding","text":"<p>               Bases: <code>Module</code></p> <p>Rotary Position Embedding module.</p> <p>Uses Triton kernel on CUDA when available, otherwise falls back to PyTorch.</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>Dimension of each attention head (head_dim).</p> required <code>max_seq_len</code> <code>int</code> <p>Maximum sequence length to cache.</p> <code>2048</code> <code>base</code> <code>float</code> <p>Base for frequency computation (default: 10000).</p> <code>10000.0</code> Example <p>rope = RotaryEmbedding(dim=32, max_seq_len=128) q = torch.randn(2, 16, 4, 32) k = torch.randn(2, 16, 4, 32) q_rot, k_rot = rope(q, k)</p> Source code in <code>src/rotalabs_accel/kernels/rope.py</code> <pre><code>class RotaryEmbedding(torch.nn.Module):\n    \"\"\"\n    Rotary Position Embedding module.\n\n    Uses Triton kernel on CUDA when available, otherwise falls back to PyTorch.\n\n    Args:\n        dim: Dimension of each attention head (head_dim).\n        max_seq_len: Maximum sequence length to cache.\n        base: Base for frequency computation (default: 10000).\n\n    Example:\n        &gt;&gt;&gt; rope = RotaryEmbedding(dim=32, max_seq_len=128)\n        &gt;&gt;&gt; q = torch.randn(2, 16, 4, 32)\n        &gt;&gt;&gt; k = torch.randn(2, 16, 4, 32)\n        &gt;&gt;&gt; q_rot, k_rot = rope(q, k)\n    \"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        max_seq_len: int = 2048,\n        base: float = 10000.0,\n    ):\n        super().__init__()\n        self.dim = dim\n        self.max_seq_len = max_seq_len\n        self.base = base\n\n        # Build initial cache\n        cos, sin = build_rope_cache(max_seq_len, dim, base)\n        self.register_buffer(\"cos_cache\", cos, persistent=False)\n        self.register_buffer(\"sin_cache\", sin, persistent=False)\n\n    def _extend_cache(self, seq_len: int) -&gt; None:\n        \"\"\"Extend cache if sequence is longer than current cache.\"\"\"\n        if seq_len &lt;= self.cos_cache.shape[0]:\n            return\n\n        new_len = max(seq_len, self.cos_cache.shape[0] * 2)\n        cos, sin = build_rope_cache(\n            new_len,\n            self.dim,\n            self.base,\n            device=self.cos_cache.device,\n            dtype=self.cos_cache.dtype,\n        )\n        self.cos_cache = cos\n        self.sin_cache = sin\n\n    def forward(\n        self,\n        q: torch.Tensor,\n        k: torch.Tensor,\n        position_ids: Optional[torch.Tensor] = None,\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Apply RoPE to query and key tensors.\n\n        Args:\n            q: Query tensor [batch, seq, heads, head_dim]\n            k: Key tensor [batch, seq, heads, head_dim]\n            position_ids: Optional position indices [batch, seq].\n\n        Returns:\n            Tuple of (q_rotated, k_rotated).\n        \"\"\"\n        seq_len = q.shape[1]\n        self._extend_cache(seq_len)\n\n        if position_ids is None:\n            cos = self.cos_cache[:seq_len]\n            sin = self.sin_cache[:seq_len]\n        else:\n            cos = self.cos_cache[position_ids]\n            sin = self.sin_cache[position_ids]\n\n        return apply_rope(q, k, cos, sin)\n\n    def extra_repr(self) -&gt; str:\n        return f\"dim={self.dim}, max_seq_len={self.max_seq_len}, base={self.base}\"\n</code></pre>"},{"location":"api/kernels/rope/#rotalabs_accel.kernels.rope.RotaryEmbedding.__init__","title":"__init__","text":"<pre><code>__init__(\n    dim: int, max_seq_len: int = 2048, base: float = 10000.0\n)\n</code></pre> Source code in <code>src/rotalabs_accel/kernels/rope.py</code> <pre><code>def __init__(\n    self,\n    dim: int,\n    max_seq_len: int = 2048,\n    base: float = 10000.0,\n):\n    super().__init__()\n    self.dim = dim\n    self.max_seq_len = max_seq_len\n    self.base = base\n\n    # Build initial cache\n    cos, sin = build_rope_cache(max_seq_len, dim, base)\n    self.register_buffer(\"cos_cache\", cos, persistent=False)\n    self.register_buffer(\"sin_cache\", sin, persistent=False)\n</code></pre>"},{"location":"api/kernels/rope/#rotalabs_accel.kernels.rope.RotaryEmbedding.forward","title":"forward","text":"<pre><code>forward(\n    q: Tensor,\n    k: Tensor,\n    position_ids: Optional[Tensor] = None,\n) -&gt; Tuple[torch.Tensor, torch.Tensor]\n</code></pre> <p>Apply RoPE to query and key tensors.</p> <p>Parameters:</p> Name Type Description Default <code>q</code> <code>Tensor</code> <p>Query tensor [batch, seq, heads, head_dim]</p> required <code>k</code> <code>Tensor</code> <p>Key tensor [batch, seq, heads, head_dim]</p> required <code>position_ids</code> <code>Optional[Tensor]</code> <p>Optional position indices [batch, seq].</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>Tuple of (q_rotated, k_rotated).</p> Source code in <code>src/rotalabs_accel/kernels/rope.py</code> <pre><code>def forward(\n    self,\n    q: torch.Tensor,\n    k: torch.Tensor,\n    position_ids: Optional[torch.Tensor] = None,\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Apply RoPE to query and key tensors.\n\n    Args:\n        q: Query tensor [batch, seq, heads, head_dim]\n        k: Key tensor [batch, seq, heads, head_dim]\n        position_ids: Optional position indices [batch, seq].\n\n    Returns:\n        Tuple of (q_rotated, k_rotated).\n    \"\"\"\n    seq_len = q.shape[1]\n    self._extend_cache(seq_len)\n\n    if position_ids is None:\n        cos = self.cos_cache[:seq_len]\n        sin = self.sin_cache[:seq_len]\n    else:\n        cos = self.cos_cache[position_ids]\n        sin = self.sin_cache[position_ids]\n\n    return apply_rope(q, k, cos, sin)\n</code></pre>"},{"location":"api/kernels/rope/#implementation-notes","title":"Implementation Notes","text":""},{"location":"api/kernels/rope/#cache-precomputation","title":"Cache Precomputation","text":"<p>The cos/sin tables are computed once and reused:</p> <pre><code>def build_rope_cache(seq_len, head_dim, base=10000.0, device=\"cuda\"):\n    # Compute frequencies\n    inv_freq = 1.0 / (base ** (torch.arange(0, head_dim, 2) / head_dim))\n\n    # Compute position angles\n    t = torch.arange(seq_len, device=device)\n    freqs = torch.outer(t, inv_freq)\n\n    # Cache cos and sin\n    cos = torch.cos(freqs)\n    sin = torch.sin(freqs)\n\n    return cos, sin\n</code></pre>"},{"location":"api/kernels/rope/#memory-layout","title":"Memory Layout","text":"<p>The rotation is applied to pairs of adjacent dimensions:</p> <ul> <li>\\((x_0, x_1)\\) rotated by \\(\\theta_0\\)</li> <li>\\((x_2, x_3)\\) rotated by \\(\\theta_1\\)</li> <li>etc.</li> </ul> <p>This \"interleaved\" layout matches LLaMA and most modern models. Some older models use \"sequential\" layout where first half and second half are paired.</p>"},{"location":"api/kernels/rope/#extended-context-yarn-ntk","title":"Extended Context (YaRN, NTK)","text":"<p>For extended context lengths, you can modify the base frequency:</p> <pre><code># NTK-aware scaling for 4x context extension\nbase = 10000 * 4.0\n\nrope = RotaryEmbedding(dim=128, max_seq_len=32768, base=base)\n</code></pre>"},{"location":"api/kernels/rope/#references","title":"References","text":"<ul> <li>RoFormer: Enhanced Transformer with Rotary Position Embedding - Original RoPE paper</li> <li>LLaMA: Open and Efficient Foundation Language Models - Uses RoPE</li> <li>YaRN: Efficient Context Window Extension - Context extension with RoPE</li> </ul>"}]}